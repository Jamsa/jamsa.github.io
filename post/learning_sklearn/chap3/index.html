<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Learning Sklearn笔记（三） - Jamsa&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Jamsa" /><meta name="description" content="非监督学习 从前一章我们可以看到即监督学习方法可以在已有数据基础上判断将来的数据，但也存在明显的缺点：数据必须是准备过的；必须人工对一定的样本" /><meta name="keywords" content="java, python, emacs" />






<meta name="generator" content="Hugo 0.78.1 with theme even" />


<link rel="canonical" href="http://jamsa.github.io/post/learning_sklearn/chap3/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Learning Sklearn笔记（三）" />
<meta property="og:description" content="非监督学习 从前一章我们可以看到即监督学习方法可以在已有数据基础上判断将来的数据，但也存在明显的缺点：数据必须是准备过的；必须人工对一定的样本" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://jamsa.github.io/post/learning_sklearn/chap3/" />
<meta property="article:published_time" content="2018-01-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-01-05T00:00:00+00:00" />
<meta itemprop="name" content="Learning Sklearn笔记（三）">
<meta itemprop="description" content="非监督学习 从前一章我们可以看到即监督学习方法可以在已有数据基础上判断将来的数据，但也存在明显的缺点：数据必须是准备过的；必须人工对一定的样本">
<meta itemprop="datePublished" content="2018-01-05T00:00:00+00:00" />
<meta itemprop="dateModified" content="2018-01-05T00:00:00+00:00" />
<meta itemprop="wordCount" content="6117">



<meta itemprop="keywords" content="python,machine learn,jupyter notebook," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Learning Sklearn笔记（三）"/>
<meta name="twitter:description" content="非监督学习 从前一章我们可以看到即监督学习方法可以在已有数据基础上判断将来的数据，但也存在明显的缺点：数据必须是准备过的；必须人工对一定的样本"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Jamsa&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">首页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Jamsa&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">首页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Learning Sklearn笔记（三）</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-01-05 </span>
        <div class="post-category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 机器学习 </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#非监督学习">非监督学习</a>
      <ul>
        <li><a href="#主成份分析pca">主成份分析（PCA)</a></li>
        <li><a href="#使用k-means进行手写识别">使用k-means进行手写识别</a></li>
        <li><a href="#其它聚类方法">其它聚类方法</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h1 id="非监督学习">非监督学习</h1>
<p>从前一章我们可以看到即监督学习方法可以在已有数据基础上判断将来的数据，但也存在明显的缺点：数据必须是准备过的；必须人工对一定的样本进行标记。这项工作通常需要由专家来完成（比如，为了正确的分类莺尾花，需要一些了解这些花的人），这将需要耗费一些时间和金钱，并且不会产生大量的数据（至少不能与Internet上的数据相比！）。每个监督学习方法都需要构建在尽可能多的样本数据上。</p>
<p>但是某些情况下我们可能没有标记过的数据。比如在婚礼上安排座位，你想将某类人分在同一桌（新娘的家人，新郎的朋友等），这个任务在机器学习中被称为“聚类”（Clustering）。有些人可能属于多个分组，你需要决定不那么相似的人是否能放在一起（比如新娘和新郎的父母）。聚类需要找出所有元素中相类似的一组元素，但是不同分组中的元素是不同的。如何定义相似是聚类方法需要回答的问题。另一个重要的问题是如何分割簇。人类非常善于从二维数据中找出聚簇（比如从地图中根据出现的街道标识出城市），但是当维度增加时情况会更复杂。</p>
<p>本章将聚焦于多个聚类方法：k-means、affinity propagation、mean shift、Gaussian Mixture Models。</p>
<p>非监督学习的另一个方面是降维（Dimentionality Reduction）。假如我们需要展现学习样本的大量属性并将它们的主要特性展示出来。当特征数量超过3时就非常困难了，因为我们不能展示3维以上的模型。降维提供了一种手段，让我们可以将高维数据在低维空间中展现出来，并保留（至少是部分的保留）它们的模式结构。这些方法也有助于我们进行模型的选择。比如，是否应该在某些监督学习任务中使用线性超平面或其它更复杂的模型。</p>
<h2 id="主成份分析pca">主成份分析（PCA)</h2>
<p>PCA是一种正交线性转换算法，它将一组相关的变量集转换为一组不相关的变量集。新的变量位于新的坐标系中，通过将数据投影到第一个坐标轴得到最大的变化，投影到第二个坐标轴得到第二大的变化，依次类推。这些新的坐标轴被称为主成分，我们可以有与原始数据维数相同多的主成人，但是我们只保留那些有高方差的。每个新的主成人被添加到主成份集中，但它必须遵守应该与其它主成份正交性（不相关）的限定。PCA可以被看作是一种揭示数据内部结构的一种方法；它提供了原始数据在低维空间上的投影。如果我们只保留第一个主成分，数据维度被重建我们可以比较方便的将数据的结构展示出来。如果包含前两个主成分，则可以在二维图上展示。PCA可以方便我们在构建预测模型前观察和分析数据。</p>
<p>对于机器学习方法而言，PCA可以在尽可能保留高方差的情况下将高维空间在低维重建。它是一个非监督方法，因为它在转换时不需要目目标值；它只依赖于学习的样本属性值。这对于以下两个目标非常有用：</p>
<ul>
<li>
<p>可视化：高维数据映射到二维图。</p>
</li>
<li>
<p>特征选择：将高维样本映射到低维，可以避免维数灾难。将样本使用PCA转换至低维，在这个新的空间中使用机器学习算法。</p>
</li>
</ul>
<p>本节以手写识别样本数据为例，样本为 8x8 像素的图像，即每个样本有64个属性。64维的数据不能可视化，因此可以使用PCA将它转换为2维进行可视化处理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>

<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="n">X_digits</span> <span class="p">,</span><span class="n">y_digits</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span>
<span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>['images', 'data', 'target_names', 'DESCR', 'target']
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="n">n_row</span><span class="p">,</span><span class="n">n_col</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span>

<span class="k">def</span> <span class="nf">print_digits</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">max_n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1">#设置figure大小</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">n_col</span><span class="p">,</span><span class="mf">2.26</span> <span class="o">*</span> <span class="n">n_row</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span> <span class="n">max_n</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">n_row</span><span class="p">,</span><span class="n">n_col</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span><span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
        
        <span class="n">p</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">,</span>
                 <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
        
        <span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">print_digits</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="n">max_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap3_files/chap3_2_0.png" alt="png"></p>
<p>定义一个函数绘制PCA转换后的二维散点图。数据点将会被根据它的分类加上颜色。样本中的目标值，不会参与转换；但我们需要了解进行PCA转换后了解他们的分布情况。我们使用10种不同的颜色来表示这10个数字。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_pca_scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">):</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span><span class="s1">&#39;white&#39;</span><span class="p">,</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="s1">&#39;lime&#39;</span><span class="p">,</span><span class="s1">&#39;cyan&#39;</span><span class="p">,</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">colors</span><span class="p">)):</span>
        <span class="n">px</span> <span class="o">=</span> <span class="n">X_pca</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">y_digits</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">py</span> <span class="o">=</span> <span class="n">X_pca</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">y_digits</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">px</span><span class="p">,</span><span class="n">py</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;First Principal Component&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Second Principal Component&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
</code></pre></td></tr></table>
</div>
</div><p>接下来我们需要进行PCA转换。在sklearn中，PCA被实现为通过<code>fit</code>方法学习一定数量样本后产生的转换对象。有好几种类型的实现不同类型PCA分解器，比如PCA、ProbalilisticPCA、RandomizePCA、KernelPCA。这里，我们使用 sklearn.decomposition 模块中的 PCA 类。最重要的参数是 n_components，通过它可以指定所获得的对象应有的特征数量。这里，我们需要将64个特征转换为2个特征，因此 n_components 需要设置为 2。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_digits</span><span class="p">)</span>
<span class="n">plot_pca_scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap3_files/chap3_6_0.png" alt="png"></p>
<p>译注：得到的图是书中图的垂直翻转镜像，也许是因为PCA类的实现有变化！</p>
<p>从上图，我们可以得到以下结论：</p>
<ul>
<li>
<p>可以看到对应于10个数字的10个不同的分类。其中的多数分类的聚簇是比较清晰的，它们的簇间有一定的距离。除了数字5对应的样本分布比较稀疏，与其它分类存在重叠。</p>
</li>
<li>
<p>另一个极端是数字0对应的分类，它的聚簇是最为独立的。凭直觉可以认为这类样本应该是最容易与其它类型区分开的；如果我们训练分类器，则它的分类结果应该是最好的。</p>
</li>
<li>
<p>从局部分布来看，我们可以预测邻近的分类对应于相类似的数字，它们比较难于区分。比如，数字9和3的簇是相邻的，可能从3中分离出9会比从4中分离出9更困难。</p>
</li>
</ul>
<p>我们迅速的从图中获取到了一些信息。这种方式可以用在训练监督模的分类器前，用于更理解所面临的困难。使用这些知识，我们可以更好的对特征进行预处理，进行特征选择，选择一个更合适的学习模型。如前面提到的，这也可以帮助我们进行维度的重建，避免陷入维度灾难，也可以使我们使用更简单的方法，比如线性模型来处理这些问题。</p>
<p>通过访问评估器的 <code>components_</code> 属性，我们可以了解主成分转换器。每个主成分都是一个矩阵，用于将原始空间中的向量转换至目标空间。而在上面的散点图中，我们只能看到两个成分。</p>
<p>我们可以按原始数据同样的形状（shape），绘制出所有的成分。(译注：这里是以图像形式绘制各个主成分转换矩阵，与原始的图像没有直接关系)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">print_pca_components</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">n_col</span><span class="p">,</span><span class="n">n_row</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">n_col</span><span class="p">,</span><span class="mf">2.26</span> <span class="o">*</span> <span class="n">n_row</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_row</span><span class="p">,</span><span class="n">n_col</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span>
                   <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;-componet&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">print_pca_components</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap3_files/chap3_8_0.png" alt="png"></p>
<p>译注：得到的图与书中也是相反的，这可能是导致上面的散点图像与书中相比是垂直翻转镜像的原因。书中深浅区域与上图中的刚好相反，上图中深蓝区域，在书中是深红色。会导致下面的部分描述也需要相反的方式描述。</p>
<p>通过查看头两个成分的图，我们可以发现一些有趣的信息：</p>
<ul>
<li>
<p>从第二个主成分图中，我们可以看出图像的中心区域是最暗的。数字图像中与之最类似的是0，因为它的中心区域是空的。通过查看前面的散点图可以确认这个直觉。查看数字0对应的簇可以看到它对应于第二个主成分的值是比较高的。</p>
</li>
<li>
<p>从第一个成分图中可以看到，如散点图中看到的，它对于分割数字4（左边值高）和3（右边值低）对应的簇是非常有效的。这个区域中的低地（蓝色）区域与数字3非常像，但它的高亮部分（黄色）与数字4非常像。</p>
</li>
</ul>
<p>如果我们使用额外的主成分，我们将获取更多和特征以便将样本分到新的维度。比如，我们可以添加第三个主成分描绘出3维散点图。</p>
<h2 id="使用k-means进行手写识别">使用k-means进行手写识别</h2>
<p>K-means是最流行的聚类算法，因为它简单，易于实现且在不同任务中都有较好的性能表现。它属于分区聚类算法，这类算法同时将分区数据放入不同的被称为簇的分组中。另一类，本书不会涉及的是层次聚类算法，这类算法先找到初始的簇，之后将它分离或者合并到新的簇。</p>
<p>K-means算法的主要思想是找到数据的分区，比如让簇均值（中心）与每个点的距离平方值最小化。需要注意的是，这类算法假设定你知道你的数据应该被划分的簇的数量（分类的数量）。</p>
<p>用k-means实现手写识别：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_digits</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">max_n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span><span class="o">&lt;</span> <span class="n">max_n</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span><span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
        <span class="n">p</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">print_digits</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="n">max_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>


</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap3_files/chap3_10_0.png" alt="png"></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">images_train</span><span class="p">,</span><span class="n">images_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n_samples</span><span class="p">,</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
<span class="n">n_digits</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">y_train</span>

</code></pre></td></tr></table>
</div>
</div><p>训练数据准备好后，接下来就是将样本进行分簇。K-means是按下面的步骤处理的：</p>
<ol>
<li>
<p>随机选择初始的簇中心</p>
</li>
<li>
<p>找到离每个样本最近的簇中心，并将样本归于这个簇</p>
</li>
<li>
<p>计算簇数据点的均值作为新的簇中心。重复进行这个处理直到簇的成员稳定；即每次迭代时只有少量样本的簇标签发生变化</p>
</li>
</ol>
<p>由k-means的工作方式可知，它有可能陷入局部极小值，且簇中心的初始值对分类的影响非常大。减轻这一问题最常用的方式是尝试多次，选择样本至簇心（或惯性中心）距离平方和最小的一组。sklearn中的k-means实现已经包含了这个功能（n-init参数可以设置尝试的次数）。它也允许我们指定分割数据的初始重心，以便获取更好的结果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',
    random_state=42, tol=0.0001, verbose=0)
</code></pre>
<p>这个过程与之前的监督学习类似，但是fit方法只需要接收训练数据，而没有分类标签。并且在调用时需要指定簇的数量。这里我们知道这个值。</p>
<p>如果我们打印出分类器的<code>labels_</code>属性，将得到每个簇心与各个样本间的距离的list。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">print_digits</span><span class="p">(</span><span class="n">images_train</span><span class="p">,</span><span class="n">clf</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span><span class="n">max_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap3_files/chap3_15_0.png" alt="png"></p>
<p>这里的簇编号与实际的数值并没有关系。因为我们并不是使用分类；而只是按图像的相似性进行分组。</p>
<p>然后我们可以使用<code>predict</code>方法对训练集进行分类。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_cluster</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">cluster_number</span><span class="p">):</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">y_pred</span><span class="o">==</span><span class="n">cluster_number</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">y_pred</span><span class="o">==</span><span class="n">cluster_number</span><span class="p">]</span>
    <span class="n">print_digits</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">max_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">print_cluster</span><span class="p">(</span><span class="n">images_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap3_files/chap3_17_0.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_1.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_2.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_3.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_4.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_5.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_6.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_7.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_8.png" alt="png"></p>
<p><img src="../learning_sklearn/chap3_files/chap3_17_9.png" alt="png"></p>
<p>上面的代码显示了每个簇最多10个图像。一些簇是非常清晰的，比如簇编号2对应的图像。</p>
<p>簇编号2对应于0，编号7的效果则并不太好，编号9只有6个样本。</p>
<p>需要明白的是，我们并不是在进行图像的分类（如前一章的人脸识别）。我们是在划分10个分组（可以修改参数调整分组的数量）。</p>
<p>如何评估其性能呢？准确度等方法无法使用，因为我们没有目标值与其进行对比。我们需要知道“实际”的簇，即它的含义。就我们的例子来说，我们可以假定每次绘制都包含了相同的数量。通过这一点，我们可以计算我们的聚类分配和预期值之间的兰德指数（Adjusted Rand Index）。兰德指数类似于精度的概念，但它考虑了这样一个事实，即，类别在不同的聚类分配方式中，可以有不同的名字。如果我们修改了类别名，其索引值不会变。调整后的指数试图从偶然发生的结果巧合中扣除。当两个集合上的簇完全相同时，兰德指数为1，没有簇共享节点时，指数为0。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;调整兰德分值：{:.2}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&#34;混淆矩阵&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>调整兰德分值：0.57
混淆矩阵
[[ 0  0 43  0  0  0  0  0  0  0]
 [20  0  0  7  0  0  0 10  0  0]
 [ 5  0  0 31  0  0  0  1  1  0]
 [ 1  0  0  1  0  1  4  0 39  0]
 [ 1 50  0  0  0  0  1  2  0  1]
 [ 1  0  0  0  1 41  0  0 16  0]
 [ 0  0  1  0 44  0  0  0  0  0]
 [ 0  0  0  0  0  1 34  1  0  5]
 [21  0  0  0  0  3  1  2 11  0]
 [ 0  0  0  0  0  2  3  3 40  0]]
</code></pre>
<p>译注：行是测试标记值，列为预测值。行是0-9。列不是有序的。
观察测试集上的数字0的分类，它被分配在簇编号2（第3列）。对数字8的分类是有问题的：21个实例被分配在编号0上，但有11个分配在编号8。效果并不好。</p>
<p>如果想在图形方式显示k-means聚簇，我们必须将它画在2维平面上。之前已经学习过PCA。我们可以用降维后的数据构建数据点的meshgrid（网格采样点），计算出他们的簇并绘制出来。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">reduced_X_train</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># 背景网格，用于标识出各个簇的范围</span>
<span class="c1"># 网格的步长</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># 网格的范围</span>
<span class="n">x_min</span><span class="p">,</span><span class="n">x_max</span> <span class="o">=</span> <span class="n">reduced_X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">reduced_X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span><span class="n">y_max</span> <span class="o">=</span> <span class="n">reduced_X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">reduced_X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span>

<span class="c1"># 生成网格采样点</span>
<span class="n">xx</span><span class="p">,</span><span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span><span class="n">x_max</span><span class="p">,</span><span class="n">h</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span><span class="n">y_max</span><span class="p">,</span><span class="n">h</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># 训练数据</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_digits</span><span class="p">,</span><span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">reduced_X_train</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="k">print</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;----&#39;</span><span class="p">)</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="c1"># 范围网格数据</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span>
           <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span><span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span><span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span>
           <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span><span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">)</span>

<span class="c1">#绘制训练集中的点</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">reduced_X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">reduced_X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;k.&#39;</span><span class="p">,</span><span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># 绘制重心为白点</span>
<span class="n">centroids</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centroids</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">centroids</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">169</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;K-means clustering on the digits dataset (PCA reduced data)</span><span class="se">\n</span><span class="s1"> Centroids are marked with white dots&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span><span class="n">y_max</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span><span class="n">y_max</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>(1514, 1592)
[8 8 8 ..., 2 2 2]
----
[[8 8 8 ..., 9 9 9]
 [8 8 8 ..., 9 9 9]
 [8 8 8 ..., 9 9 9]
 ..., 
 [1 1 1 ..., 2 2 2]
 [1 1 1 ..., 2 2 2]
 [1 1 1 ..., 2 2 2]]
</code></pre>
<p><img src="../learning_sklearn/chap3_files/chap3_21_1.png" alt="png"></p>
<h2 id="其它聚类方法">其它聚类方法</h2>
<p>Sklearn中包含有多个聚类方法，它们的使用方法和参数与k-means类似。本节简要描述这些方法，并给指出它们各自的优点。</p>
<p>多数聚类方法需要指定簇的数量。通常解决这个问题的方法是尝试不同的数量，并使用维度重构的方式将结果可视化，并由专家确定分类的结果是否合适。也有些方法试图自动计算出簇的数量。比如sklearn包含的亲合力传播（Affinity Propagation）算法实现，这个方法从样本中找出最具代表性的部分，并用它们来描述簇。比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">aff</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">AffinityPropagation</span><span class="p">()</span>
<span class="n">aff</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">aff</span><span class="o">.</span><span class="n">cluster_centers_indices_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>(112,)
</code></pre>
<p>亲合力传播算法从训练集中检测出了112个簇。可以使用前面定义的<code>print_digits</code>函数，将各个簇的分组情况打印出来。它的<code>cluster_centers_indices_</code>属性包含了算法找到的每个簇对应的元素。</p>
<p>另一个能计算簇数量的是MeanShift。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ms</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">MeanShift</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>(18, 64)
</code></pre>
<p>它找到了18个簇。<code>cluster_centers_</code>属性显示了簇超平面的重心。</p>
<p>注意，上面这两个方法不能使用兰德指数进行评估，因为我们没有一个规范的集合来比较。但是，我们可以测量聚类的惯性，因为惯性是每个数据点到质心距离的总和；我们希望数字接近0。不幸的是，sklearn中除k-means外，其它方法不能测量惯性。</p>
<p>最后，我们将尝试用概率方法进行聚类，从程序的角度来看，它与k-means非常相似，但是它们的理论原理是完全不同的。 GMM假定数据服从具有未知参数的高斯混合分布。 高斯分布是用于对许多现象进行建模的统计学中众所周知的分布函数。它具有以均值为中心的钟形函数;你可能已经看过以下的图：</p>
<p><img src="%7Battach%7Dlearning_sklearn/gmm.png" alt="高斯混合模型" title="高斯混合模型"></p>
<p>如果我们足够多的男性身高数据，直方图将服从均值1.774米标准差0.1466米的高斯分布。均值标识出多数可能的值（与曲线的峰值一致），标准差表示结果如何分散；距离平均值多远。如果我们测量两个指定身高间曲线下方的面积（即，它的积分）。如果分布正确，给定一个人，我们可以知道它的身高在给定的两个指定身高之间的概率是多少。我们为什么要期侍这种分布，而不是其它呢？实际上，并不是每个现象都有相同的分布，但是大数定律告诉我们，无论何时我们重复一个实验达到足够多的次数时（例如，测量某人的身高），结果的分布可以近似为高斯发布。</p>
<p>一般来说，我们会是一个多元（即涉及多个特征）分布，但是方法是相同的。超平面上的一个点（均值），大多数样本会更接近它；当我们偏离中间值时，在簇中找到样本的概率将会降低。这个概率下降多少取决于第二个参数，即方差。正如我们所说的，GMM假设每个簇都是一个多元正态分布，方法的目标是找到k个质心（使用称期望最大化（EM）的算法估算训练数据的均值和方差），并将每个点分配到最近的均值。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">mixture</span>

<span class="n">gm</span> <span class="o">=</span> <span class="n">mixture</span><span class="o">.</span><span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_digits</span><span class="p">,</span>
                 <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;tied&#39;</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><pre><code>GaussianMixture(covariance_type='tied', init_params='kmeans', max_iter=100,
        means_init=None, n_components=10, n_init=1, precisions_init=None,
        random_state=42, reg_covar=1e-06, tol=0.001, verbose=0,
        verbose_interval=10, warm_start=False, weights_init=None)
</code></pre>
<p>过程与使用k-means类似。<code>covariance_type</code>参数指定我们的期望特征；即，每个像素相关。比如，我们可以假设它们是不相关的，但是我们希望相近的点是相关的。这里，我们使用绑定的协方差类型。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Adjusted rand score:{:.2}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Homogeneity score:{:.2}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">homogeneity_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Completeness score:{:.2}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">completeness_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>Adjusted rand score:0.46
</code></pre>
<p>与k-means相比，得到了更好的兰德分值（译注：我这里是更差了）。上面也包含了sklearn.metrics中两个新的评价方法。同质性（Homogeneity）是一个0.0和1.0间的数值（越大越好）。数值1.0表示簇的数据点都是属于单一分类的，表明聚类效果非常好。完整性（Completeness）是当给定分类的所有点都在同样的簇中（对所有样本进行了有效的分类，而不是构建出多个小而独立的簇）。我们可以把同质性和完整性当作非监督学习中的精度和召回率（recall）。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">Jamsa</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2018-01-05
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/python/">python</a>
          <a href="/tags/machine-learn/">machine learn</a>
          <a href="/tags/jupyter-notebook/">jupyter notebook</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/numpy/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Numpy的使用</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/learning_sklearn/chap2/">
            <span class="next-text nav-default">Learning Sklearn笔记（二）</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="http://github.com/jamsa" class="iconfont icon-github" title="github"></a>
  <a href="http://jamsa.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2020<span class="heart"><i class="iconfont icon-heart"></i></span><span>Jamsa</span>
  </span>
</div>
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>








</body>
</html>
