<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Learning Sklearn笔记（二） - Jamsa&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Jamsa" /><meta name="description" content="Chap2 监督学习 使用SVM进行图像识别 SVM通过选择分享不同样例的最宽分割超平面来进行分类处理。 通过使用非线性平面，例如 polynomial 或 rbf(radial basis function) 核函数将样本映射" /><meta name="keywords" content="java, python, emacs" />






<meta name="generator" content="Hugo 0.78.1 with theme even" />


<link rel="canonical" href="http://jamsa.github.io/post/learning_sklearn/chap2/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Learning Sklearn笔记（二）" />
<meta property="og:description" content="Chap2 监督学习 使用SVM进行图像识别 SVM通过选择分享不同样例的最宽分割超平面来进行分类处理。 通过使用非线性平面，例如 polynomial 或 rbf(radial basis function) 核函数将样本映射" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://jamsa.github.io/post/learning_sklearn/chap2/" />
<meta property="article:published_time" content="2018-01-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-01-05T00:00:00+00:00" />
<meta itemprop="name" content="Learning Sklearn笔记（二）">
<meta itemprop="description" content="Chap2 监督学习 使用SVM进行图像识别 SVM通过选择分享不同样例的最宽分割超平面来进行分类处理。 通过使用非线性平面，例如 polynomial 或 rbf(radial basis function) 核函数将样本映射">
<meta itemprop="datePublished" content="2018-01-05T00:00:00+00:00" />
<meta itemprop="dateModified" content="2018-01-05T00:00:00+00:00" />
<meta itemprop="wordCount" content="11112">



<meta itemprop="keywords" content="python,machine learn,jupyter notebook," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Learning Sklearn笔记（二）"/>
<meta name="twitter:description" content="Chap2 监督学习 使用SVM进行图像识别 SVM通过选择分享不同样例的最宽分割超平面来进行分类处理。 通过使用非线性平面，例如 polynomial 或 rbf(radial basis function) 核函数将样本映射"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Jamsa&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">首页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Jamsa&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">首页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Learning Sklearn笔记（二）</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-01-05 </span>
        <div class="post-category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 机器学习 </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#chap2-监督学习">Chap2 监督学习</a>
      <ul>
        <li><a href="#使用svm进行图像识别">使用SVM进行图像识别</a></li>
        <li><a href="#使用朴素贝叶斯进行文本分类">使用朴素贝叶斯进行文本分类</a>
          <ul>
            <li><a href="#数据预处理">数据预处理</a></li>
            <li><a href="#训练贝叶斯分类器">训练贝叶斯分类器</a></li>
            <li><a href="#性能评估">性能评估</a></li>
          </ul>
        </li>
        <li><a href="#使用决策树解释泰坦尼克假设">使用决策树解释泰坦尼克假设</a>
          <ul>
            <li><a href="#数据处理">数据处理</a></li>
            <li><a href="#训练决策树分类器">训练决策树分类器</a></li>
            <li><a href="#解释决策树">解释决策树</a></li>
            <li><a href="#随机森林随机化决策">随机森林——随机化决策</a></li>
            <li><a href="#性能评估-1">性能评估</a></li>
          </ul>
        </li>
        <li><a href="#使用拟合进行房价预测">使用拟合进行房价预测</a>
          <ul>
            <li><a href="#第一次尝试-线性模型">第一次尝试 线性模型</a></li>
            <li><a href="#第二次尝试-支持向量机">第二次尝试 支持向量机</a></li>
            <li><a href="#第三次尝试-重访随机森林">第三次尝试 重访随机森林</a></li>
            <li><a href="#评估">评估</a></li>
            <li><a href="#小结">小结</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h1 id="chap2-监督学习">Chap2 监督学习</h1>
<h2 id="使用svm进行图像识别">使用SVM进行图像识别</h2>
<p>SVM通过选择分享不同样例的最宽分割超平面来进行分类处理。</p>
<p>通过使用非线性平面，例如 polynomial 或 rbf(radial basis function) 核函数将样本映射到高维特征空间对非线性分布的数据进行分类。</p>
<p>SVM非常适合于高维或稀疏的样本，内存消耗也比较为有效，因为在学习和决定边界时，它只使用样本的子集。</p>
<p>需要注意的是SVM模型训练是计算密集型的。用它进行分类时不会返回数值型的置信度值，虽然我们可以使用交叉验证法避免这个问题，但随之而来的是更高的计算开销。</p>
<p>下面的示例是使用SVM对人脸进行分类处理：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span> 
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="kn">as</span> <span class="nn">sk</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_olivetti_faces</span>

<span class="c1">#获取数据集</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_olivetti_faces</span><span class="p">()</span>

<span class="k">print</span> <span class="n">faces</span><span class="o">.</span><span class="n">DESCR</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>Modified Olivetti faces dataset.

The original database was available from

    http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

The version retrieved here comes in MATLAB format from the personal
web page of Sam Roweis:

    http://www.cs.nyu.edu/~roweis/

There are ten different images of each of 40 distinct subjects. For some
subjects, the images were taken at different times, varying the lighting,
facial expressions (open / closed eyes, smiling / not smiling) and facial
details (glasses / no glasses). All the images were taken against a dark
homogeneous background with the subjects in an upright, frontal position (with
tolerance for some side movement).

The original dataset consisted of 92 x 112, while the Roweis version
consists of 64x64 images.
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span> <span class="n">faces</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>['images', 'data', 'target', 'DESCR']
</code></pre>
<p>images是400,64,64的图像，data是400*4096经过归一化处理后的图像数据</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">print_faces</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">target</span><span class="p">,</span><span class="n">top_n</span><span class="p">):</span>
    <span class="c1">#打印图像</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">top_n</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span><span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
        <span class="n">p</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bone</span><span class="p">)</span>
        
        <span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

<span class="n">print_faces</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">faces</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="mi">20</span><span class="p">);</span>        

</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap2_files/chap2_4_0.png" alt="png"></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1">#线性核</span>
<span class="n">svc_1</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>

<span class="c1">#分割数据集</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">faces</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span><span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">sem</span>

<span class="k">def</span> <span class="nf">evalute_cross_validation</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
    <span class="c1">#创建K拆交叉验证迭代</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="n">K</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1">#默认使用评估难器（精度）的分值</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">scores</span>
    <span class="c1">#平均分（精度）,</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Mean score: {0: .3f} (+/-{1:.3f})&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span><span class="n">sem</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
    
<span class="n">evalute_cross_validation</span><span class="p">(</span><span class="n">svc_1</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><pre><code>/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  &quot;This module will be removed in 0.20.&quot;, DeprecationWarning)


[ 0.93333333  0.86666667  0.91666667  0.93333333  0.91666667]
Mean score:  0.913 (+/-0.012)
</code></pre>
<p>使用5折交叉验证的结果是精度0.933，还算不错。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="k">def</span> <span class="nf">train_and_evaluate</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;训练集上的精度:&#34;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;测试集上的精度&#34;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;分类器报告：&#34;</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;混淆矩阵&#34;</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
    
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">svc_1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
<span class="c1">#help(cross_val_score)</span>

</code></pre></td></tr></table>
</div>
</div><pre><code>训练集上的精度:
1.0
测试集上的精度
0.99
分类器报告：
             precision    recall  f1-score   support

          0       0.86      1.00      0.92         6
          1       1.00      1.00      1.00         4
          2       1.00      1.00      1.00         2
          3       1.00      1.00      1.00         1
          4       1.00      1.00      1.00         1
          5       1.00      1.00      1.00         5
          6       1.00      1.00      1.00         4
          7       1.00      0.67      0.80         3
          9       1.00      1.00      1.00         1
         10       1.00      1.00      1.00         4
         11       1.00      1.00      1.00         1
         12       1.00      1.00      1.00         2
         13       1.00      1.00      1.00         3
         14       1.00      1.00      1.00         5
         15       1.00      1.00      1.00         3
         17       1.00      1.00      1.00         6
         19       1.00      1.00      1.00         4
         20       1.00      1.00      1.00         1
         21       1.00      1.00      1.00         1
         22       1.00      1.00      1.00         2
         23       1.00      1.00      1.00         1
         24       1.00      1.00      1.00         2
         25       1.00      1.00      1.00         2
         26       1.00      1.00      1.00         4
         27       1.00      1.00      1.00         1
         28       1.00      1.00      1.00         2
         29       1.00      1.00      1.00         3
         30       1.00      1.00      1.00         4
         31       1.00      1.00      1.00         3
         32       1.00      1.00      1.00         3
         33       1.00      1.00      1.00         2
         34       1.00      1.00      1.00         3
         35       1.00      1.00      1.00         1
         36       1.00      1.00      1.00         3
         37       1.00      1.00      1.00         3
         38       1.00      1.00      1.00         1
         39       1.00      1.00      1.00         3

avg / total       0.99      0.99      0.99       100

混淆矩阵
[[6 0 0 ..., 0 0 0]
 [0 4 0 ..., 0 0 0]
 [0 0 2 ..., 0 0 0]
 ..., 
 [0 0 0 ..., 3 0 0]
 [0 0 0 ..., 0 1 0]
 [0 0 0 ..., 0 0 3]]
</code></pre>
<p>经过训练后的分类器几乎没有错误。</p>
<p>下面使用分类器分类戴眼镜或不戴眼镜的图像。</p>
<p>下面这些图像是戴眼镜的图像索引范围（如：10-19，30-32这些索引范围的都是戴眼镜的）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">glasses</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">19</span><span class="p">),</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">),</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">59</span><span class="p">),</span> <span class="p">(</span><span class="mi">63</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
      <span class="p">(</span><span class="mi">69</span><span class="p">,</span> <span class="mi">69</span><span class="p">),</span> <span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">121</span><span class="p">),</span> <span class="p">(</span><span class="mi">124</span><span class="p">,</span> <span class="mi">129</span><span class="p">),</span> <span class="p">(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">139</span><span class="p">),</span> <span class="p">(</span><span class="mi">160</span><span class="p">,</span> <span class="mi">161</span><span class="p">),</span>
      <span class="p">(</span><span class="mi">164</span><span class="p">,</span> <span class="mi">169</span><span class="p">),</span> <span class="p">(</span><span class="mi">180</span><span class="p">,</span> <span class="mi">182</span><span class="p">),</span> <span class="p">(</span><span class="mi">185</span><span class="p">,</span> <span class="mi">185</span><span class="p">),</span> <span class="p">(</span><span class="mi">189</span><span class="p">,</span> <span class="mi">189</span><span class="p">),</span> <span class="p">(</span><span class="mi">190</span><span class="p">,</span> <span class="mi">192</span><span class="p">),</span>
      <span class="p">(</span><span class="mi">194</span><span class="p">,</span> <span class="mi">194</span><span class="p">),</span> <span class="p">(</span><span class="mi">196</span><span class="p">,</span> <span class="mi">199</span><span class="p">),</span> <span class="p">(</span><span class="mi">260</span><span class="p">,</span> <span class="mi">269</span><span class="p">),</span> <span class="p">(</span><span class="mi">270</span><span class="p">,</span> <span class="mi">279</span><span class="p">),</span> <span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">309</span><span class="p">),</span>
      <span class="p">(</span><span class="mi">330</span><span class="p">,</span> <span class="mi">339</span><span class="p">),</span> <span class="p">(</span><span class="mi">358</span><span class="p">,</span> <span class="mi">359</span><span class="p">),</span> <span class="p">(</span><span class="mi">360</span><span class="p">,</span> <span class="mi">369</span><span class="p">)</span>
<span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><p>创建用于将戴眼镜和不戴眼镜的标识为1和0的目标数据：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">
<span class="k">def</span> <span class="nf">create_target</span><span class="p">(</span><span class="n">segments</span><span class="p">):</span>
    <span class="c1">#创建一个新的y数组，大小于target相同，默认值全部为0</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#将glasses中指定索引范围的元素设置为1</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span><span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="n">segments</span><span class="p">:</span>
        <span class="n">y</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="n">target_glasses</span> <span class="o">=</span> <span class="n">create_target</span><span class="p">(</span><span class="n">glasses</span><span class="p">)</span>

<span class="c1">#分割训练和测试集</span>
<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                 <span class="n">target_glasses</span><span class="p">,</span>
                                                 <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                                                 <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">svc_2</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">evalute_cross_validation</span><span class="p">(</span><span class="n">svc_2</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>[ 1.          0.95        0.98333333  0.98333333  0.93333333]
Mean score:  0.970 (+/-0.012)
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">svc_2</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>训练集上的精度:
1.0
测试集上的精度
0.99
分类器报告：
             precision    recall  f1-score   support

        0.0       1.00      0.99      0.99        67
        1.0       0.97      1.00      0.99        33

avg / total       0.99      0.99      0.99       100

混淆矩阵
[[66  1]
 [ 0 33]]
</code></pre>
<p>如何确定这个分类器能用于它未见过的人脸呢？我们可以从样本中分离出10个来（30,39)，用于评估。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X_test</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">30</span><span class="p">:</span><span class="mi">40</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">target_glasses</span><span class="p">[</span><span class="mi">30</span><span class="p">:</span><span class="mi">40</span><span class="p">]</span>
<span class="k">print</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">select</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">target_glasses</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1">#选择30:40之外的，值为1的数据</span>
<span class="n">select</span><span class="p">[</span><span class="mi">30</span><span class="p">:</span><span class="mi">40</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">select</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">target_glasses</span><span class="p">[</span><span class="n">select</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="k">print</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">svc_3</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">svc_3</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>10
390
训练集上的精度:
1.0
测试集上的精度
0.9
分类器报告：
             precision    recall  f1-score   support

        0.0       0.83      1.00      0.91         5
        1.0       1.00      0.80      0.89         5

avg / total       0.92      0.90      0.90        10

混淆矩阵
[[5 0]
 [1 4]]
</code></pre>
<p>精度0.9表明存在一个错误的分类。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">svc_3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">eval_faces</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,(</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">))</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">]</span>
<span class="n">print_faces</span><span class="p">(</span><span class="n">eval_faces</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap2_files/chap2_16_0.png" alt="png"></p>
<p>第8个图像识别错了。</p>
<h2 id="使用朴素贝叶斯进行文本分类">使用朴素贝叶斯进行文本分类</h2>
<p>朴素贝叶斯是从贝叶斯定理中产生的一个简单而强大的概率模型。它通过属于某个分类的实例的每个特征值的概率来决定实例的分类概率。朴素一词指它认为各个特征之间是无关联的。</p>
<p>模型相当于在计算条件概率。</p>
<p>朴素贝叶斯最为成功的应用是在自然语言处理（NLP）领域。NLP问题通常会从文档中抽取大量标签值数据，并以此为基础进行训练。</p>
<p>用来学习的数据集为20个不同主题的19000条新闻，主题包含：政治、宗教、运动、科学。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#%pylab inline</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">news</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="k">print</span> <span class="nb">type</span><span class="p">(</span><span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">),</span><span class="nb">type</span><span class="p">(</span><span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">),</span><span class="nb">type</span><span class="p">(</span><span class="n">news</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="k">print</span> <span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span> <span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">news</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>&lt;type 'list'&gt; &lt;type 'numpy.ndarray'&gt; &lt;type 'list'&gt;
From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;
Subject: Pens fans reactions
Organization: Post Office, Carnegie Mellon, Pittsburgh, PA
Lines: 12
NNTP-Posting-Host: po4.andrew.cmu.edu



I am sure some bashers of Pens fans are pretty confused about the lack
of any kind of posts about the recent Pens massacre of the Devils. Actually,
I am  bit puzzled too and a bit relieved. However, I am going to put an end
to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they
are killing those Devils worse than I thought. Jagr just showed you why
he is much better than his regular season stats. He is also a lot
fo fun to watch in the playoffs. Bowman should let JAgr have a lot of
fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final
regular season game.          PENS RULE!!!


10 rec.sport.hockey
</code></pre>
<h3 id="数据预处理">数据预处理</h3>
<p>这里需要将文本转换为数据集。可以使用 sklearn.feature_extraction.text 模块从文本文档中提取特征向量。</p>
<p>在转换前，我们需要先划分训练集和测试集。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">SPLIT_PERC</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">split_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="n">SPLIT_PERC</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">split_size</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">split_size</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="n">split_size</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">split_size</span><span class="p">:]</span>
</code></pre></td></tr></table>
</div>
</div><p><code>sklearn.feature_extraction.text</code> 有三种不同的将文本转换为数值特征的方法：<code>CountVectorizer</code>,<code>HashingVectorizer</code>,<code>TfidfVectorizer</code>。<code>CountVectorizer</code> 从文本全集中创建词典，然后将它转化为一个向量，向量中每个元素的值是这个词在某个文档中出现的次数。<code>HashingVectorizer</code>不同的是它不在内存中维护字典，而是通过哈希算法将词映射为索引值，然后使用<code>CountVectorizer</code>的方式进行计数。<code>TfidfVectorizier</code>与<code>CountVectorizer</code>也是类似的，但它使用的是更先进的计算方法<code>TF-IDF</code>（Term Frequency Inverse Document Frequency），它统计了词在单个文档或所有文档全集中的重要性。比如，它检查词在当前文档中出现有频率与在所有文档集中出现的频率的比值。这种方式能更好的处理归一化的问题，避免样本的词过高的问题。</p>
<h3 id="训练贝叶斯分类器">训练贝叶斯分类器</h3>
<p>下面将使用<code>Pipeline</code>来组合<code>MultinomiaNB</code>和三种不同的分词方式进行对比。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span><span class="p">,</span><span class="n">HashingVectorizer</span><span class="p">,</span><span class="n">CountVectorizer</span>

<span class="n">clf_1</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span><span class="n">CountVectorizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span><span class="n">MultinomialNB</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">clf_2</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span><span class="n">HashingVectorizer</span><span class="p">(</span><span class="n">non_negative</span><span class="o">=</span><span class="bp">True</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span><span class="n">MultinomialNB</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">clf_3</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span><span class="n">TfidfVectorizer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span><span class="n">MultinomialNB</span><span class="p">())</span>
<span class="p">])</span>

<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span><span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">sem</span>

<span class="k">def</span> <span class="nf">evaluate_cross_validation</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
    <span class="c1">#k拆交叉验证</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="n">K</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">scores</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Mean score: {0:.3f} (+/-{1:.3f})&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span><span class="n">sem</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>

<span class="n">clfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">clf_1</span><span class="p">,</span><span class="n">clf_2</span><span class="p">,</span><span class="n">clf_3</span><span class="p">]</span>
<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">clfs</span><span class="p">:</span>
    <span class="n">evaluate_cross_validation</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>[ 0.85782493  0.85725657  0.84664367  0.85911382  0.8458477 ]
Mean score: 0.853 (+/-0.003)


/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)
/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.
  &quot; in version 0.21.&quot;, DeprecationWarning)


[ 0.75543767  0.77659857  0.77049615  0.78508888  0.76200584]
Mean score: 0.770 (+/-0.005)
[ 0.84482759  0.85990979  0.84558238  0.85990979  0.84213319]
Mean score: 0.850 (+/-0.004)
</code></pre>
<p>从上面的结果来看CountVectorizer和TfidfVectorizer有相似的性能，它们的效果比HashingVectorizer要好。</p>
<p>下面我们继续使用TfidfVectorizer，并通过调整解析文档所用的正则表达式来提升效果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf_4</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span><span class="n">TfidfVectorizer</span><span class="p">(</span>
    <span class="n">token_pattern</span> <span class="o">=</span> <span class="sa">ur</span><span class="s2">&#34;\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b&#34;</span><span class="p">,</span>
    <span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span><span class="n">MultinomialNB</span><span class="p">()),</span>
<span class="p">])</span>

<span class="n">evaluate_cross_validation</span><span class="p">(</span><span class="n">clf_4</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>[ 0.86100796  0.8718493   0.86203237  0.87291059  0.8588485 ]
Mean score: 0.865 (+/-0.003)
</code></pre>
<p>默认的正则分词正则表达式为<code>ur&quot;\b\w\w+\b&quot;</code>只能匹配字母数字和下划线，调整后的正则表达式能匹配类似 Wi-Fi 和 site.com 这样的词，因此它的精度提升了。</p>
<p>另外我们也可能使用<code>stop_words</code>参数：通过它传递一个单词的列表，这个列表中的单词将不会被统计，比如出现太频繁的词，或我们不希望出现的词。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_stop_words</span><span class="p">():</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;stopwords_en.txt&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="n">result</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">clf_5</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span><span class="n">TfidfVectorizer</span><span class="p">(</span>
        <span class="n">stop_words</span> <span class="o">=</span> <span class="n">get_stop_words</span><span class="p">(),</span>
        <span class="n">token_pattern</span> <span class="o">=</span> <span class="sa">ur</span><span class="s2">&#34;\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b&#34;</span><span class="p">,</span>
    <span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span><span class="n">MultinomialNB</span><span class="p">()),</span>
<span class="p">])</span>

<span class="n">evaluate_cross_validation</span><span class="p">(</span><span class="n">clf_5</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>[ 0.86498674  0.87397188  0.86442027  0.87609445  0.86256301]
Mean score: 0.868 (+/-0.003)
</code></pre>
<p><code>注意</code>:此处的stopwords_en.txt中的内容是我手加的，但是仍然可以看到精度有较少的提升。</p>
<p>接下来看<code>MultinomialNB</code>的参数。这个分类器有少量参数可以调整；最重要的是<code>alpha</code>参数，它是一个平滑度参数，我们可以试试将它从1（默认值），调整为0.01。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf_7</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span><span class="n">TfidfVectorizer</span><span class="p">(</span>
        <span class="n">stop_words</span> <span class="o">=</span> <span class="n">get_stop_words</span><span class="p">(),</span>
        <span class="n">token_pattern</span> <span class="o">=</span> <span class="sa">ur</span><span class="s2">&#34;\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b&#34;</span><span class="p">,</span>
    <span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span><span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)),</span>
<span class="p">])</span>

<span class="n">evaluate_cross_validation</span><span class="p">(</span><span class="n">clf_7</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>[ 0.91962865  0.919342    0.91721942  0.9265057   0.91801539]
Mean score: 0.920 (+/-0.002)
</code></pre>
<p>精度立即有了较大提升。我们可以在此基础上，继续尝试调整分词器的参数和alpha参数。</p>
<h3 id="性能评估">性能评估</h3>
<p>当我们已经对模型进行充分提升后，就可以在测试集上评估其性能。</p>
<p>我们将定义一个辅助函数，它将在整个训练集上进行训练，然后在测试集上评估精度。并打印出分类报告（在每个分类上的精度和recall）及对应的混淆矩阵：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="k">def</span> <span class="nf">train_and_evaluate</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;训练集精度：&#34;</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;测试集精度：&#34;</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;分类器报告：&#34;</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;混淆矩阵：&#34;</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
    
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">clf_7</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>训练集精度：
0.996816187916
测试集精度：
0.91617147708
分类器报告：
             precision    recall  f1-score   support

          0       0.95      0.88      0.91       216
          1       0.83      0.84      0.84       246
          2       0.91      0.83      0.87       274
          3       0.80      0.86      0.83       235
          4       0.87      0.90      0.89       231
          5       0.89      0.92      0.90       225
          6       0.88      0.80      0.84       248
          7       0.93      0.93      0.93       275
          8       0.96      0.98      0.97       226
          9       0.97      0.94      0.96       250
         10       0.97      1.00      0.98       257
         11       0.96      0.98      0.97       261
         12       0.90      0.91      0.91       216
         13       0.94      0.96      0.95       257
         14       0.94      0.96      0.95       246
         15       0.90      0.97      0.93       234
         16       0.90      0.97      0.94       218
         17       0.97      0.99      0.98       236
         18       0.95      0.90      0.93       213
         19       0.87      0.76      0.81       148

avg / total       0.92      0.92      0.92      4712

混淆矩阵：
[[190   0   0   0   1   0   0   0   0   1   0   0   0   1   0   9   2   0
    0  12]
 [  0 207   5   4   3  13   4   0   0   0   0   1   3   2   3   0   0   1
    0   0]
 [  0  12 228  23   1   5   1   0   1   0   0   0   0   0   1   0   1   0
    1   0]
 [  0   6   7 201  10   3   4   0   0   0   0   0   3   0   1   0   0   0
    0   0]
 [  0   2   3   5 208   1   4   0   0   0   2   0   5   0   1   0   0   0
    0   0]
 [  0   9   2   1   1 206   0   1   1   0   0   0   0   2   1   0   0   1
    0   0]
 [  0   2   3   9   6   0 198  14   0   2   1   2   6   2   2   0   0   1
    0   0]
 [  0   3   0   1   1   0   7 255   4   1   0   0   0   1   0   0   2   0
    0   0]
 [  0   0   0   0   0   1   1   2 221   0   0   0   0   1   0   0   0   0
    0   0]
 [  0   1   0   0   0   0   1   0   2 236   5   0   1   2   0   1   1   0
    0   0]
 [  0   0   0   1   0   0   0   0   0   0 256   0   0   0   0   0   0   0
    0   0]
 [  0   0   0   0   0   1   0   0   0   0   0 255   0   1   0   0   3   0
    1   0]
 [  0   1   0   2   5   1   3   1   0   2   1   1 196   2   1   0   0   0
    0   0]
 [  0   1   0   1   1   0   0   0   0   0   0   2   1 246   3   0   1   0
    0   1]
 [  0   3   0   0   1   0   1   0   0   0   0   0   0   1 237   0   1   0
    1   1]
 [  1   0   1   2   0   0   0   1   0   0   0   1   1   0   1 226   0   0
    0   0]
 [  0   0   1   0   0   0   1   0   1   0   0   1   0   0   0   0 212   0
    2   0]
 [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 234
    1   0]
 [  1   0   0   0   0   0   1   0   0   0   0   2   1   1   0   1   7   4
  192   3]
 [  9   0   0   0   0   1   0   0   0   1   0   0   0   0   0  14   5   1
    4 113]]
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#字典中词的数量</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clf_7</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;vect&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()))</span>
<span class="c1">#字典的内容</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">clf_7</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;vect&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;sand&#39;</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>146044
[u'sand', u'sandals', u'sandalwood', u'sandbag', u'sandbags', u'sandbanks.cosc.brocku.ca', u'sandberg', u'sandblasting', u'sandcastles', u'sandels', u'sander', u'sanders', u'sanderson', u'sandfort', u'sandhu', u'sandia', u'sandia.llnl.gov', u'sandiego', u'sandiego.ncr.com', u'sanding', u'sandinista', u'sandinistas', u'sandlak', u'sandland', u'sandlot', u'sandman', u'sandman.caltech.edu', u'sandman.cs.hut.fi', u'sandman.ece.clarkson.edu', u'sandoval', u'sandpoint', u'sandra', u'sandra.l.denton', u'sandro', u'sands', u'sandstorm', u'sandstrom', u'sandstrom..22', u'sandvik', u'sandvik-140493185034', u'sandvik-140493185248', u'sandvik-140493230024', u'sandvik-140493233557', u'sandvik-150493144638', u'sandvik-150493181533', u'sandvik-160493205355', u'sandvik-160493205451', u'sandvik-170493104312', u'sandvik-170493104859', u'sandvik-180493131125', u'sandvik-190493200323', u'sandvik-190493200420', u'sandvik-190493200858', u'sandvik-190493201048', u'sandvik-190493224221', u'sandvik-200493000159', u'sandvik-200493232227', u'sandvik-200493233434', u'sandvik-200493235610', u'sandvik-210493014635', u'sandvik-210493213823', u'sandvik-210493225738', u'sandvik-210493230542', u'sandvik-250493163828', u'sandvik-kent.apple.com', u'sandwich', u'sandwiched', u'sandwiches', u'sandy', u'sandy47']
</code></pre>
<p>从上面的结果可以看出，字典中包含了类似sandbag，sandbags这样的语义上相近的词。它们应该被分入同一个桶（bucket）中，使用词干技术处理具有同样词根的单词是非常常见的工作。</p>
<h2 id="使用决策树解释泰坦尼克假设">使用决策树解释泰坦尼克假设</h2>
<p>一个比较常见的针对线性分类和统计学习方法的争议是很难解释模型是如何确定分类的。如果有一个高维的SVM模型，人们很难想像它的超平面是什么样的。朴素贝叶斯分类器则告诉你：“假如它来自于与训练数据相类似的分布，则这个分类的可能性最大”。</p>
<p>决策树是一种简单而强大的监督学习方法，使用它构造模型可用于预测。</p>
<p>下面的代码主要解决的问题是：根据泰坦尼克乘客的信息，预测他们是否会幸存。数据来自于：http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt 。每行的数据格式为：&ldquo;1&rdquo;,&ldquo;1st&rdquo;,1,&ldquo;Allen, Miss Elisabeth Walton&rdquo;,29.0000,&ldquo;Southampton&rdquo;,&ldquo;StLouis, MO&rdquo;,&ldquo;B-5&rdquo;,&ldquo;24160 L221&rdquo;,&ldquo;2&rdquo;,&ldquo;female&rdquo;。分别代表序号、仓位等级、是否生存（0为否，1为是）、姓名、年龄、登船港口、家/目的地、房号、票号、船号、性别。首先我们需要使用numpy读取csv文件。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">csv</span><span class="o">,</span><span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">load_titanic_data</span><span class="p">():</span>
    <span class="n">titanic_X</span><span class="p">,</span> <span class="n">titanic_y</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[],[],[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;titanic.csv&#39;</span><span class="p">,</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvfile</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;titanic.csv&#39;</span><span class="p">))</span>
        <span class="n">titanic_reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">csvfile</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span><span class="n">quotechar</span><span class="o">=</span><span class="s1">&#39;&#34;&#39;</span><span class="p">)</span>
        
        <span class="c1">#标题列</span>
        <span class="n">row</span> <span class="o">=</span> <span class="n">titanic_reader</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
        <span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
        
        <span class="c1">#数据集和标记</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">titanic_reader</span><span class="p">:</span>
            <span class="n">titanic_X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
            <span class="n">titanic_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        
    <span class="k">return</span> <span class="n">feature_names</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">titanic_y</span><span class="p">)</span>

<span class="n">feature_names</span><span class="p">,</span><span class="n">titanic_X</span><span class="p">,</span><span class="n">titanic_y</span> <span class="o">=</span> <span class="n">load_titanic_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">titanic_y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

</code></pre></td></tr></table>
</div>
</div><pre><code>/Users/zhujie/Documents/devel/python/machine-learn/learning_sklearn/titanic.csv
['row.names' 'pclass' 'survived' 'name' 'age' 'embarked' 'home.dest' 'room'
 'ticket' 'boat' 'sex']
(array(['1', '1st', '1', 'Allen, Miss Elisabeth Walton', '29.0000',
       'Southampton', 'St Louis, MO', 'B-5', '24160 L221', '2', 'female'],
      dtype='|S62'), '1')
</code></pre>
<h3 id="数据处理">数据处理</h3>
<p>首先我们需要提取用于学习的属性：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#只使用仓位，年龄，性别</span>
<span class="n">titanic_X</span> <span class="o">=</span> <span class="n">titanic_X</span><span class="p">[:,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">]]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">feature_names</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">]]</span>
<span class="k">print</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>[['1st' '29.0000' 'female']
 ['1st' ' 2.0000' 'female']
 ['1st' '30.0000' 'male']
 ..., 
 ['3rd' 'NA' 'male']
 ['3rd' 'NA' 'female']
 ['3rd' 'NA' 'male']]
['pclass' 'age' 'sex']
</code></pre>
<p>特征选择是创建机器学习模型极其重要的一步。无论我们有多好的机器学习算法，如果算法没有获取到好的输入特征，它就得不到足够的学习素材，使用的效果也就会不够好。</p>
<p>有时特征选择有可能会是手工进行的，这取决于我们在这个领域的知识和我们选择的机器学习算法。有时我们也能使用自动化工具来对特征进行评估。在这里我们是手工选择的。某些特征有可能会导致过拟合（如选择姓名后在决策树能直接给出是否幸存）。</p>
<p>查看上面的数据中<code>NA</code>值是缺少的年龄值。因此我们需要处理数据集中常见的缺失值的问题。在这里我们使用训练集中的年龄均值来替换缺失值。我们也可以使用不同的方式来处理，比如取训练集中最常见的值或中值。在使用代替值时要非常小心，应该清楚的知道所修改的内容，避免歪曲最终的结果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ages</span> <span class="o">=</span> <span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">#年龄均值</span>
<span class="n">mean_age</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[</span><span class="n">ages</span> <span class="o">!=</span> <span class="s1">&#39;NA&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">))</span>
<span class="c1">#缺失值处理</span>
<span class="n">titanic_X</span><span class="p">[</span><span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;NA&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_age</span>

</code></pre></td></tr></table>
</div>
</div><p>Sklearn中的决策树需要输入是实数特征值，模型的决策规则是下面的形式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">Feature &lt;= value
</code></pre></td></tr></table>
</div>
</div><p>比如<code>age &lt;= 20</code>。而我们的属性值是分类值，如性别，对应于男/女。因此，我们需要将它们转换为实数值。可以使用sklearn中的<code>LabelEncoder'类来处理，它的</code>fit<code>方法可以将分类值转换至</code>0..K-1<code>之间的整数，</code>K`是数据集中分类的数量。（这里性别值只有0或1）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#Encode set</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Categorical classes:</span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

<span class="n">integer_classes</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Integer classes:</span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">integer_classes</span><span class="p">))</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span> <span class="c1"># 将sex属性的值转换为0-1值，并修改至训练集</span>
<span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span>

<span class="k">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[</span><span class="mi">12</span><span class="p">],</span><span class="n">titanic_y</span><span class="p">[</span><span class="mi">12</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>Categorical classes:['female' 'male']
Integer classes:[0 0 1 ..., 1 0 1]
['pclass' 'age' 'sex']
(array(['1st', '31.1941810427', '0'],
      dtype='|S62'), '1')
</code></pre>
<p>另一个分类属性是：class。可以使用相似的方式转换为0，1，2。这个转换会隐式的对分类增加排序，在这里不会产生问题。但是面对更好的方式是不产生这个顺序，且能将广泛的分类转换为实数值。这里可以使用另一个编码器将这class属性转换为3个新的特征，每个特征都用0,1值表示。这种编码方式被称为<code>one hot</code>编码，它是一种将分类属性转换为实数值的常用方法。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;仓位分类：</span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

<span class="n">integer_classes</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&#34;整数分类值：</span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span><span class="p">(</span><span class="n">integer_classes</span><span class="p">))</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>

<span class="n">one_hot_encoder</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">integer_classes</span><span class="p">)</span>
<span class="n">num_of_rows</span> <span class="o">=</span> <span class="n">titanic_X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 第一步：使用label_encoder将仓位(class)属性转换为0-(N-1)的整数值</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_of_rows</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 第二步：创建一个三列的稀疏矩阵，它的每列表示一个分类，用于标识class属性是否属于该分类</span>
<span class="n">new_features</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="c1"># 将新特征添加至titanix_X</span>
<span class="n">titanic_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">titanic_X</span><span class="p">,</span><span class="n">new_features</span><span class="o">.</span><span class="n">toarray</span><span class="p">()],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 移除转换过的列</span>
<span class="n">titanic_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">,[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 更新特征名称</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span><span class="s1">&#39;first_class&#39;</span><span class="p">,</span><span class="s1">&#39;second_class&#39;</span><span class="p">,</span><span class="s1">&#39;third_class&#39;</span><span class="p">]</span>

<span class="c1"># 转换为数值</span>
<span class="n">titanic_X</span> <span class="o">=</span> <span class="n">titanic_X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">titanic_y</span> <span class="o">=</span> <span class="n">titanic_y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="si">%s</span><span class="s2"> </span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">titanic_X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">titanic_y</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>仓位分类：['1st' '2nd' '3rd']
整数分类值：[[0]
 [1]
 [2]]
['age', 'sex', 'first_class', 'second_class', 'third_class']
[ 29.   0.   1.   0.   0.] 1.0
</code></pre>
<p>现在数据集已经可以用来训练决策树了。并且，归一化对决策树来说不是问题，因为特征的相对大小对于分类没有影响。</p>
<p>预处理步骤在机器学习方法中经常被低估了，但是我们应该看到即使是在这个非常简单的例子中，仍然需要花些时间来让数据满足训练方法的要求。它在机器学习过程中是非常重要的，无论我们选择多好的学习方法，如果这一步失败，后续步骤一样会失败。</p>
<h3 id="训练决策树分类器">训练决策树分类器</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">titanic_X</span><span class="p">,</span><span class="n">titanic_y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">33</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><p><code>DecisionTreeClassifier</code>可接收多个超参数。这里使用信息增益（Information Gain)来判断训练数据的分割，并告诉分类器最多构建3层，且每个叶节点至少包含5个训练实例。为了解释决策树，我们可以将构建的模型可视化。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pydot</span><span class="o">,</span><span class="nn">StringIO</span>
<span class="n">dot_data</span> <span class="o">=</span> <span class="n">StringIO</span><span class="o">.</span><span class="n">StringIO</span><span class="p">()</span>

<span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">out_file</span><span class="o">=</span><span class="n">dot_data</span><span class="p">,</span><span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span><span class="s1">&#39;1s_class&#39;</span><span class="p">,</span><span class="s1">&#39;2nd_class&#39;</span><span class="p">,</span><span class="s1">&#39;3rd_class&#39;</span><span class="p">])</span>

<span class="p">(</span><span class="n">graph</span><span class="p">,)</span> <span class="o">=</span> <span class="n">pydot</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>
<span class="n">graph</span><span class="o">.</span><span class="n">write_png</span><span class="p">(</span><span class="s1">&#39;titanic.png&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;titanic.png&#39;</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="../learning_sklearn/chap2_files/chap2_47_0.png" alt="png"></p>
<p>对样例分类时，我们需要回答每个节点上的问题。比如，对于根节点，问题是：<code>sex&lt;-0.5</code>（是否为女性）。如果答案为是，则走向左边的子树，否则走向右边的子树。继续回答下去直到达到叶节点。这时，样例被归类为实例最多（针对前一节点的问题）的这一类。</p>
<p>如何确定在每个节点上应该询问什么问题呢？答案是“信息增益”（或使用sklearn中使用的另一个度量方式“基尼系数”）。信息增益用于度量我们回答问题后损失了多少熵。<code>熵</code>用于度量集合的混合程度，如果熵为0，则将意味着所有值是相同的（在我们的示例中，同一目标分类下的值是相同的）。当两个分类的样本数量相同时熵达到最大值（比如一半样例为幸存，另一半死亡）。</p>
<h3 id="解释决策树">解释决策树</h3>
<p>在树的开始位置共有984个实例，662个死亡，332个幸存。初始熵为0.912。从可能的问题列表中挑选出产生最大熵的问题是：他是否为女性？如果答案为是，熵几乎是相同的，如果为否则它的值变化较大（男性伤亡的比例更高）。</p>
<p>每个节点上都有：问题、香农熵、样本数量和分布。  每一步样本数量都会根据问题的答案分配到左边或右边。直至遇到结束条件（在这里是直到第四层，或样本的数量少于5个）。</p>
<p>在使用模型进行判断时，我们拿样本来遍历树节点，并回答各个节点的问题，直到到达叶节点。这时，我们检查训练集上每个分类的样本数量，并选择样本数量较多的分类作为要判断的样本的分类。</p>
<p>例如，判断一个头等仓的10岁女孩是否能幸存。首先，回答第一个问题（是否为女性？）答案为是，因此进入左树。接下来回答为否（是否为3等仓），是（是否为头等仓）。这时到达了叶节点，在训练时这个节点有102个样本，其中97个幸存，因此最终的答案是幸存。（译注：书中的树结构与上面生成的结构不同）</p>
<p>我们也能感觉到答案的合理性：伤亡者多数是二等或三等仓中的男性（从树中样本可以看到）。多数头等仓中的女孩都幸存了。我们可以检测下模型在训练集上的精度：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="k">def</span> <span class="nf">measure_performance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span> <span class="n">show_accurary</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_classification_report</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_confussion_matrix</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_accurary</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;精度：{0:.3f}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">show_classification_report</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;分类器报告：&#34;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">show_confussion_matrix</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;混淆矩阵&#34;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>

<span class="n">measure_performance</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">show_classification_report</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">show_confussion_matrix</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><pre><code>精度：0.838
</code></pre>
<p>我们的决策树在训练集上的精度为0.838。但这并不是个好的指标，特别是对于决策树容易出现过拟合。因为我们并没有划分验证集，且应该使用交叉验证。比如，使用交叉验证的一种特例，留一法（leave-one-out cross-validation）。对于训练集中的每个样本，我们用其余样本训练模型，并用这个保留样本评估模型。在执行与训练样本数量一样多的分类之后，我们根据预测保留样本类别的次数的比例来计算准确度，会发现它比之前的方式的值要低（这是适合预期的，因为之前的方式容易过拟合，它是在训练集上进行验证的）。</p>
<p>译注 ：
留一法交叉验证：假设有N个样本，将每一个样本作为测试样本，其它N-1个样本作为训练样本。这样得到N个分类器，N个测试结果。用这N个结果的平均值来衡量模型的性能。</p>
<p>普通交叉验证：我理解的是K倍交叉验证（k-fold cross validation）：将所有样本分成K份，一般每份样本的数量相等或相差不多。取一份作为测试样本，剩余K-1份作为训练样本。这个过程重复K次，最后的平均测试结果可以衡量模型的性能。</p>
<p><a href="https://www.zhihu.com/question/23561944">https://www.zhihu.com/question/23561944</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">LeaveOneOut</span> 
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">sem</span>

<span class="k">def</span> <span class="nf">loo_cv</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">clf</span><span class="p">):</span>
    <span class="n">loo</span> <span class="o">=</span> <span class="n">LeaveOneOut</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span><span class="n">test_index</span> <span class="ow">in</span> <span class="n">loo</span><span class="p">:</span>
        <span class="n">X_train_cv</span><span class="p">,</span><span class="n">X_test_cv</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
        <span class="n">y_train_cv</span><span class="p">,</span><span class="n">y_test_cv</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span><span class="n">y_train</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
        <span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_cv</span><span class="p">,</span><span class="n">y_train_cv</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_cv</span><span class="p">)</span>
        <span class="n">scores</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_cv</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span><span class="n">y_pred</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Mean score: {0:.3f} (+/-{1:.3f})&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span><span class="n">sem</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>

<span class="n">loo_cv</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">clf</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>Mean score: 0.837 (+/-0.012)
</code></pre>
<p>使用留一交叉难法的好处是它能让绝大部分数据用于训练，因此特别适合于样本数据比较少的情况。它的主要问题是需要针对每个样本训练一个分类器会需要大量的计算。</p>
<h3 id="随机森林随机化决策">随机森林——随机化决策</h3>
<p>决策树存在一个问题，在回答问题之后训练集会被拆分，就不会再参与到后续的决策。比如，当数据集按性别拆分后，后面的问题将只与男性或女性相关，而无法处理与性别无关的问题（如，年龄小于一岁的与性别无关）。随机森林试图在每步中引入一定的随机化，用替代树组合以获取最终的预测值。这类考虑多个分类器回答相同问题的方式被称为集成方法（ensemble methods）。在上面预测幸存的任务中，难以使用这种算法，因为它的特征太少了。</p>
<p>随机森林随机选择训练集的一个子集来训练决策树，且只使用随机选择的部分特征。经过多次训练后树数量增加，产生了一套分类器。在进行预测时，每棵树都对样本进行预测，最终的结果则由组合分类器根据所有树的预测结果投票决定。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">33</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">loo_cv</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">clf</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>Mean score: 0.817 (+/-0.012)
</code></pre>
<p>引入随机会后，看起来效果变差了，因为特征的数量太少。当处理的数据特征和样本数量都比较多时，随机森林可以在保留决策树优点的情况下，以非常快速、简单的方式来提升精度。并且，在下节我们将使用它来进行拟合。</p>
<h3 id="性能评估-1">性能评估</h3>
<p>每种监督学习任务的最后一步都是在未使用过的数据集上对分类器进行评估，以确定它的预测能力。这一步不应该使用已经。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf_dt</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf_dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">measure_performance</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">clf_dt</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>精度：0.793
分类器报告：
             precision    recall  f1-score   support

        0.0       0.77      0.96      0.85       202
        1.0       0.88      0.54      0.67       127

avg / total       0.81      0.79      0.78       329

混淆矩阵
[[193   9]
 [ 59  68]]
</code></pre>
<h2 id="使用拟合进行房价预测">使用拟合进行房价预测</h2>
<p>将拟合看作可以无限分类的分类。</p>
<p>本节将对同一数据集使用多种拟合方法。通过使用样本的属性预测出它的价格。数据集包含506个样本，每个样本14个特征，特征的值都是实数值。</p>
<p>sklearn中包含了这些样本数据。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;最高价：</span><span class="si">%s</span><span class="s2"> 最低价：</span><span class="si">%s</span><span class="s2"> 平均价：</span><span class="si">%s</span><span class="s2">&#34;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="c1">#print(boston.DESCR)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>(506, 13)
['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'
 'B' 'LSTAT']
最高价：50.0 最低价：5.0 平均价：22.5328063241
</code></pre>
<p>查看<code>boston.DESCR</code>来了解数据集各个特征。这是需要重要的，我们需要通过理解数据来选择合适的模型。</p>
<p>分割数据集，并进行归一化：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">33</span><span class="p">)</span>
<span class="c1">#y_train = [y_train]</span>
<span class="c1">#y_test = [y_test]</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scalerX</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">scalery</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scalerX</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">scalery</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scalerX</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">scalery</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

</code></pre></td></tr></table>
</div>
</div><p>如何对预测的结果进行评估呢？之前我们用过交叉验证，但是精度值并不适合，因为这里我们预测的值是实数值。sklearn针对这种情况提供了多种度量方式，最常用的<code>R2</code>值，或决定系数（coef cient of determination）度量模型所解释的结果的变化比例，这是sklearn中拟合方法默认使用的评估方法。当分值达到最大值1时，模型的与目标值最为匹配。通过这种方式，我们定义一个方法训练模型，并采用五折交叉验证和决定系数（coef）来评估其性能。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="o">*</span>
<span class="k">def</span> <span class="nf">train_and_evaluate</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;训练集上的决定系数：</span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)))</span>
    <span class="c1">#五折交叉验证</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">5</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">33</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;使用五折交叉验证的平均决定系数：</span><span class="si">%s</span><span class="s2">&#34;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="第一次尝试-线性模型">第一次尝试 线性模型</h3>
<p>线性模型试图回答的问题是在由学习特征（包括目标值）组成的14维空间中哪个超平面与它们更接近。当找到这个超平面后，通过将要预测的点投影到超平面，并返回这个点在目标轴（目标值所在维）上坐标值来实现预测。</p>
<p>但是什么是接近呢？通常使用最小二乘：计算每个样本距离超平面的距离，并平方（避免正符符号问题），并求和。超平面就是最小二乘评估器的较小值。</p>
<p>下面使用SGDRegressor这一线性模型，它通过最小化平方误差来实现。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="n">clf_sgd</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDRegressor</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_loss&#39;</span><span class="p">,</span><span class="n">penalty</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">clf_sgd</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;超平面：</span><span class="si">%s</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">clf_sgd</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>训练集上的决定系数：0.743617732983
使用五折交叉验证的平均决定系数：0.710809853468
超平面：[-0.08527595  0.06706144 -0.05032898  0.10874804 -0.07755151  0.38961893
 -0.02485839 -0.20990016  0.08491659 -0.05495108 -0.19854006  0.06126093
 -0.37817963]
</code></pre>
<p>参数 penalty 参数是线性拟合方法中为避免过拟合而引入的惩罚因子。它通过检查超平面每个特征对预测值的贡献，惩罚那些超平面系数太大的情况。这个参数通常使用 <code>L2</code>（系数的平方和） 正则或 <code>L1</code>（系数的绝对值之和）正则。下面是使用L2正则。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf_sgd1</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDRegressor</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_loss&#39;</span><span class="p">,</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">clf_sgd1</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><pre><code>训练集上的决定系数：0.743616743208
使用五折交叉验证的平均决定系数：0.71081206667
</code></pre>
<p>这种情况下，并未得到提升。</p>
<h3 id="第二次尝试-支持向量机">第二次尝试 支持向量机</h3>
<p>使用SVM可以代替超平面进行拟合。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="n">clf_svr</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">clf_svr</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><pre><code>训练集上的决定系数：0.71886923342
使用五折交叉验证的平均决定系数：0.707838419194
</code></pre>
<p>仍然没的提升。但SVM的一个好处就在于我们可以使用非线性函数（核函数），比如，使用多项式函数逼近（polynomial核）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf_svr_ploy</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;poly&#39;</span><span class="p">)</span>
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">clf_svr_ploy</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>训练集上的决定系数：0.904109273301
使用五折交叉验证的平均决定系数：0.779288545488
</code></pre>
<p>现在有了提升。实际上还可以使用RBF核来提升。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf_svr_rbf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span>
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">clf_svr_rbf</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>训练集上的决定系数：0.900132065979
使用五折交叉验证的平均决定系数：0.833662221567
</code></pre>
<p>RBF核是sklearn中SVM的默认核函数。</p>
<h3 id="第三次尝试-重访随机森林">第三次尝试 重访随机森林</h3>
<p>我们可以使用随机森林来实现拟合。之前我们使用过随机森林进行分类。在用作拟合时，树的构建过程与之前类似，但是在预测时，当我们到达叶节点时，我们不返回主分类，而是返回一个实数值，比如，叶节点下样本的均值。</p>
<p>实际上我们将使用“Extra Tree”，它由模块<code>sklearn.ensemble</code>模块中的<code>ExtraTreesRegressor</code>类提供。它增加了额外的随机化处理。它不仅在每棵树的样本、选择特征时使用了随机值，在每个决策位置也引入了随机的阀值。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">ensemble</span>
<span class="n">clf_et</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">ExtraTreesRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># ,compute_importances=True不再被支持</span>
<span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">clf_et</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>/Users/zhujie/.pyenv/versions/2.7.14/envs/machinelearn/lib/python2.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  This is separate from the ipykernel package so we can avoid doing imports until


训练集上的决定系数：1.0
使用五折交叉验证的平均决定系数：0.861758978344
</code></pre>
<p>首先我们注意到不光消除了欠拟合（在训练集上完全匹配），而提高了在交叉验证上的性能。Extra Tree的一个有意思的功能是，它允许计算在拟合任务中每个特征的重要性。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">clf_et</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>[['0.00504385320276' 'AGE']
 ['0.0151425137151' 'B']
 ['0.0170525784005' 'CHAS']
 ['0.0189418210858' 'CRIM']
 ['0.0236025617776' 'DIS']
 ['0.0257330490046' 'INDUS']
 ['0.0318741622351' 'LSTAT']
 ['0.0344056449393' 'NOX']
 ['0.0397131333452' 'PTRATIO']
 ['0.0466185213973' 'RAD']
 ['0.0995118014928' 'RM']
 ['0.284215227964' 'TAX']
 ['0.35814513144' 'ZN']]
</code></pre>
<p>我们可以看到ZN属性（面积）和TAX属性（含税价）是影响最终决策的最有影响力的特征。</p>
<h3 id="评估">评估</h3>
<p>我们使用修改版的函数（这个函数会显示决策时的参数）来进行评估。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="k">def</span> <span class="nf">measure_performance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">show_accuracy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="n">show_classification_report</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">show_confussion_matrix</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="n">show_r2_score</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_accuracy</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;精度：{0:.3f}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accurary_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">show_classification_report</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;分类器报告：&#34;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">show_confussion_matrix</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;混淆矩阵&#34;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ypred</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">show_r2_score</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&#34;决策参数：{0:.3f}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>

<span class="n">measure_performance</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">clf_et</span><span class="p">,</span><span class="n">show_accuracy</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">show_classification_report</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="n">show_confussion_matrix</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">show_r2_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>决策参数：0.802
</code></pre>
<h3 id="小结">小结</h3>
<p>本章介绍了监督学习的常用方法。监督学习需要样本具备输入特征和目标值。下章接触到的非监督学习则不需要目标值。这些方法对于弄清数据的结构非常有用，且可以作为训练监督学习模型的前凑步骤。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">Jamsa</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2018-01-05
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/python/">python</a>
          <a href="/tags/machine-learn/">machine learn</a>
          <a href="/tags/jupyter-notebook/">jupyter notebook</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/learning_sklearn/chap3/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Learning Sklearn笔记（三）</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/learning_sklearn/chap4/">
            <span class="next-text nav-default">Learning Sklearn笔记（四）</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="http://github.com/jamsa" class="iconfont icon-github" title="github"></a>
  <a href="http://jamsa.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2020<span class="heart"><i class="iconfont icon-heart"></i></span><span>Jamsa</span>
  </span>
</div>
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>








</body>
</html>
