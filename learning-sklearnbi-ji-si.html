<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Jamsa" />
        <meta name="robots" content="index, follow"/>

        <meta property="og:title" content="Learning Sklearn笔记（四）"/>
        <meta property="og:url" content="./learning-sklearnbi-ji-si.html"/>
        <meta property="og:site_name" content="Jamsa的笔记"/>
        <meta property="og:type" content="article"/>

        <link rel="canonical" href="./learning-sklearnbi-ji-si.html" />

        <title>Learning Sklearn笔记（四） | Jamsa的笔记</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" />
        <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" />
        

        <link rel="stylesheet" type="text/css" href="./theme/css/main.css" />

        <script type="text/javascript">var switchTo5x=true;</script>
        <script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
        <script type="text/javascript">
         stLight.options({
             publisher: "",
             doNotHash: false,
             doNotCopy: false,
             hashAddressBar: false
         });
        </script>
    </head>

    <body id="index">
        <div class="row-fluid">
            <div class="span10 offset1">
                <header id="banner" >
                    <h1>
                        <a href="./">Jamsa的笔记 </a>
                    </h1>
                    <nav class="navbar">
                        <div class="navbar-inner">
                            <ul class="nav">
                                <li ><a href="./category/da-shu-ju.html">大数据</a></li>
                                <li ><a href="./category/fang-fa.html">方法</a></li>
                                <li class="active"><a href="./category/ji-qi-xue-xi.html">机器学习</a></li>
                                <li ><a href="./category/kai-fa.html">开发</a></li>
                                <li ><a href="./category/qian-duan.html">前端</a></li>
                                <li ><a href="./category/xiao-lu.html">效率</a></li>
                                <li ><a href="./category/yi-dong.html">移动</a></li>
                            </ul>

                        </div>
                    </nav>
                </header><!-- /#banner -->
            </div>
        </div>

        <div class="row-fluid">
            <div class="span10 offset1">
                <div class="row-fluid">
<div class="span10 offset1">
  <section>
    <article>
      <header>
        <h1 class="entry-title">
          <a href="./learning-sklearnbi-ji-si.html" rel="bookmark"
             title="Permalink to Learning Sklearn笔记（四）">Learning Sklearn笔记（四）</a></h1>
      </header>
      <div class="entry-content">
<footer class="post-info">
    <address class="vcard author">
        by <a class="url fn" href="./author/jamsa.html">Jamsa</a>
    </address>

    in <a href="./category/ji-qi-xue-xi.html">机器学习</a>

    on 2018-01-05

        |
        tags:         <a href="./tag/python.html">python</a>
        <a href="./tag/machine-learn.html">machine learn</a>
        <a href="./tag/jupyter-notebook.html">jupyter notebook</a>


        |
        <a href="./learning-sklearnbi-ji-si.html#disqus_thread">comments</a>

    
</footer><!-- /.post-info -->

        <h1>Chap4 特征处理</h1>
<p>本章主要学习：</p>
<ul>
<li>
<p>特征抽取：将现实世界的数据构建为sklearn中可用的特征数据</p>
</li>
<li>
<p>特征选择：从可用的特征数据中，选择适当的特征集。</p>
</li>
<li>
<p>模型选择：选择合适的算法和参数</p>
</li>
</ul>
<h2>特征抽取</h2>
<p>根据机器学习的需要将潜在的特征转换为学习方法所需要的格式被称为特征抽取或特征工程。通常可以分为两步：</p>
<ul>
<li>
<p>获取特征：对源数据进行处理并提取学习样本，通常用特征/值对的形式表示，值可以是整数、浮点、字符串、分类代码等。提取特征的方法则严重依赖于源数据。</p>
</li>
<li>
<p>特征转换：多数sklearn算法需要的样本集是list形式的，特征值为浮点类型。</p>
</li>
</ul>
<p>Pandas可以用于对数据的预处理，它提供了类似R语言的功能。</p>
<p>下面将使用Pandas将tatanic.csv 数据转换为pandas和DataFrame格式（DataFrame是二维标签数据，列可以包含不同的数据类型，每行表示一个样本）</p>
<div class="highlight"><pre><span></span><span class="o">%</span><span class="n">pylab</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'titanic.csv'</span><span class="p">)</span>
<span class="c1">#print(titanic)</span>
<span class="k">print</span><span class="p">(</span><span class="n">titanic</span><span class="o">.</span><span class="n">head</span><span class="p">()[[</span><span class="s1">'pclass'</span><span class="p">,</span><span class="s1">'survived'</span><span class="p">,</span><span class="s1">'age'</span><span class="p">,</span><span class="s1">'embarked'</span><span class="p">,</span><span class="s1">'boat'</span><span class="p">,</span><span class="s1">'sex'</span><span class="p">]])</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">Populating</span> <span class="n">the</span> <span class="n">interactive</span> <span class="n">namespace</span> <span class="kn">from</span> <span class="nn">numpy</span> <span class="nn">and</span> <span class="nn">matplotlib</span>
  <span class="n">pclass</span>  <span class="n">survived</span>      <span class="n">age</span>     <span class="n">embarked</span>   <span class="n">boat</span>     <span class="n">sex</span>
<span class="mi">0</span>    <span class="mi">1</span><span class="n">st</span>         <span class="mi">1</span>  <span class="mf">29.0000</span>  <span class="n">Southampton</span>      <span class="mi">2</span>  <span class="n">female</span>
<span class="mi">1</span>    <span class="mi">1</span><span class="n">st</span>         <span class="mi">0</span>   <span class="mf">2.0000</span>  <span class="n">Southampton</span>    <span class="n">NaN</span>  <span class="n">female</span>
<span class="mi">2</span>    <span class="mi">1</span><span class="n">st</span>         <span class="mi">0</span>  <span class="mf">30.0000</span>  <span class="n">Southampton</span>  <span class="p">(</span><span class="mi">135</span><span class="p">)</span>    <span class="n">male</span>
<span class="mi">3</span>    <span class="mi">1</span><span class="n">st</span>         <span class="mi">0</span>  <span class="mf">25.0000</span>  <span class="n">Southampton</span>    <span class="n">NaN</span>  <span class="n">female</span>
<span class="mi">4</span>    <span class="mi">1</span><span class="n">st</span>         <span class="mi">1</span>   <span class="mf">0.9167</span>  <span class="n">Southampton</span>     <span class="mi">11</span>    <span class="n">male</span>


<span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">zhujie</span><span class="o">/.</span><span class="n">pyenv</span><span class="o">/</span><span class="n">versions</span><span class="o">/</span><span class="mf">2.7</span><span class="o">.</span><span class="mi">14</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">machinelearn</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python2</span><span class="o">.</span><span class="mi">7</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">IPython</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">magics</span><span class="o">/</span><span class="n">pylab</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">161</span><span class="p">:</span> <span class="ne">UserWarning</span><span class="p">:</span> <span class="n">pylab</span> <span class="kn">import</span> <span class="nn">has</span> <span class="nn">clobbered</span> <span class="nn">these</span> <span class="nn">variables</span><span class="p">:</span> <span class="p">[</span><span class="s1">'clf'</span><span class="p">]</span>
<span class="sb">`%matplotlib`</span> <span class="n">prevents</span> <span class="n">importing</span> <span class="o">*</span> <span class="kn">from</span> <span class="nn">pylab</span> <span class="nn">and</span> <span class="nn">numpy</span>
  <span class="s2">"</span><span class="se">\n</span><span class="s2">`%matplotlib` prevents importing * from pylab and numpy"</span>
</pre></div>
<p>主要问题是sklearn的方法需要特征为实数值。在第二章中我们使用了LabelEncoder和OneHotEncoder预处理方法手工将分类特征转换为了1-of-K值（每个可能的值都生成为一个特征，原特征值匹配时值为1，其它特征值为0）。这次，我们使用sklarn中类似的方法DictVectorizer，它能自动的从不同的原始特征值构造这些特征。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">feature_extraction</span>
<span class="k">def</span> <span class="nf">one_hot_dataframe</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">cols</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">feature_extraction</span><span class="o">.</span><span class="n">DictVectorizer</span><span class="p">()</span>
    <span class="n">mkdict</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">row</span><span class="p">:</span><span class="nb">dict</span><span class="p">((</span><span class="n">col</span><span class="p">,</span><span class="n">row</span><span class="p">[</span><span class="n">col</span><span class="p">])</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">)</span>
    <span class="n">vecData</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span>
        <span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">mkdict</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
    <span class="n">vecData</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
    <span class="n">vecData</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">index</span>
    <span class="k">if</span> <span class="n">replace</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">vecData</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">vecData</span><span class="p">)</span>
</pre></div>
<p>one_hot_dataframe方法接收pandas DataFrame 数据结构和列头信息，将每个列转换换为1-of-K特征。如果replace参数为True，它将使用新的特征集替换掉原来的列。下面使用它来处理pclass，embarked和sex特征。</p>
<div class="highlight"><pre><span></span><span class="n">titanic</span><span class="p">,</span><span class="n">titanic_n</span> <span class="o">=</span> <span class="n">one_hot_dataframe</span><span class="p">(</span><span class="n">titanic</span><span class="p">,[</span><span class="s1">'pclass'</span><span class="p">,</span><span class="s1">'embarked'</span><span class="p">,</span><span class="s1">'sex'</span><span class="p">],</span><span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">titanic</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
<div class="highlight"><pre><span></span>         row.names     survived         age  embarked  embarked=Cherbourg  \
count  1313.000000  1313.000000  633.000000     821.0         1313.000000   
mean    657.000000     0.341965   31.194181       0.0            0.154608   
std     379.174762     0.474549   14.747525       0.0            0.361668   
min       1.000000     0.000000    0.166700       0.0            0.000000   
25%     329.000000     0.000000   21.000000       0.0            0.000000   
50%     657.000000     0.000000   30.000000       0.0            0.000000   
75%     985.000000     1.000000   41.000000       0.0            0.000000   
max    1313.000000     1.000000   71.000000       0.0            1.000000

       embarked=Queenstown  embarked=Southampton   pclass=1st   pclass=2nd  \
count          1313.000000           1313.000000  1313.000000  1313.000000   
mean              0.034273              0.436405     0.245240     0.213252   
std               0.181998              0.496128     0.430393     0.409760   
min               0.000000              0.000000     0.000000     0.000000   
25%               0.000000              0.000000     0.000000     0.000000   
50%               0.000000              0.000000     0.000000     0.000000   
75%               0.000000              1.000000     0.000000     0.000000   
max               1.000000              1.000000     1.000000     1.000000

        pclass=3rd   sex=female     sex=male  
count  1313.000000  1313.000000  1313.000000  
mean      0.541508     0.352628     0.647372  
std       0.498464     0.477970     0.477970  
min       0.000000     0.000000     0.000000  
25%       0.000000     0.000000     0.000000  
50%       1.000000     0.000000     1.000000  
75%       1.000000     1.000000     1.000000  
max       1.000000     1.000000     1.000000
</pre></div>
<p>pclass属性被转换为了3个特征：pclass=1st，pclass=2nd，pclass=3rd。embarked列并没有消失，这是因为原始的embarked属性中包含了NaN值，即缺失值，这种情况下embarked属性有值的样本的新特征值会被设置为0，NaN值仍然不变，表示这个特征是缺失的。接下来处理其分类值：</p>
<div class="highlight"><pre><span></span><span class="n">titanic</span><span class="p">,</span><span class="n">titanic_n</span> <span class="o">=</span> <span class="n">one_hot_dataframe</span><span class="p">(</span><span class="n">titanic</span><span class="p">,[</span><span class="s1">'home.dest'</span><span class="p">,</span><span class="s1">'room'</span><span class="p">,</span><span class="s1">'ticket'</span><span class="p">,</span><span class="s1">'boat'</span><span class="p">],</span><span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>我们也可惟处理缺失值，由于DecisionTreeClassifier分类器不允许有缺失值。Pandas的fillna方法可以处理缺失值。这里我们使用年龄的平均值填充缺失的年龄特征，用0填充其它缺失的特征。</p>
<div class="highlight"><pre><span></span><span class="n">mean</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">[</span><span class="s1">'age'</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">titanic</span><span class="p">[</span><span class="s1">'age'</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">titanic</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>现在除了name属性其它属性都已经进行了处理。接下来可以划分测试和训练集。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">titanic_target</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">[</span><span class="s1">'survived'</span><span class="p">]</span>
<span class="n">titanic_data</span> <span class="o">=</span> <span class="n">titanic</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'name'</span><span class="p">,</span><span class="s1">'row.names'</span><span class="p">,</span><span class="s1">'survived'</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">titanic_data</span><span class="p">,</span><span class="n">titanic_target</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">33</span><span class="p">)</span>
</pre></div>
<p>这里丢弃了name属性，因为它与生存状态无关（每个样本的值都不同）。我们将survived特征作为了目标值，因此可以从训练向量中排除。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">'entropy'</span><span class="p">)</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"精度:{0:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="err">精度</span><span class="o">:</span><span class="mf">0.833</span>
</pre></div>
<h2>特征选择</h2>
<p>到目前为止，当我们训练决策树时，我们使用了数据集中所有可用的特征。这看起来似乎比较合理，因为我们想要用尽量多的特征来构建模型。实际上，由于两个主要的原因，我们应该限制使用的特征数量：</p>
<ul>
<li>
<p>首先，对于某些方法，特别是那些会逐步重建样本数量的方法（比如决策树），有可能那些不相关的特征的偶然出现会导致不能正确的模拟问题。某些特征可能会导致模型的泛化能力不佳。另外一些特征可能高度相关，它只是增加了一些冗余信息。</p>
</li>
<li>
<p>第二个原因则比较现实。大量特征可能会显著的增加计算时间，而对分类性能提升没有帮助。特别是在进行大数据处理时，这种情况下样本和特征可能会数千倍的增加。另外，与维度灾难相关，使用包含大量特征和样本的数据集训练出一个泛化的模型是非常困难的。</p>
</li>
</ul>
<p>因此，使用数量较少的特征可能会获得更好的效果。因此我们需要一个找出最佳特征的算法。这项任务被称为特征选择，它是一个决定性的步骤。如果我们的特征太弱，则算法将返回不良的结果，不论选择的算法有多强。</p>
<p>例如在Titanic示例中，如果我们将11个特征进行1-of-K编码处理，将会增涨到581个特征。这还不构成重要的计算问题，但是考虑一下，如果像前面的示例那样，将数据集中的每个文档表示为每个词出现的次数会发生什么情况？另一个问题是决策树会过拟合。如果分支上只有少量的样本，模型的预测能力将会下降。解决这个问题的一个方法是调整模型参数（比如最大的树深度，叶节点的最小样本数量）。但是在这里，我们将采取不同的方法：我们尝试将特征限制在最相关的特征上。</p>
<p>相关性如何表示呢？这是个重要的问题。如果特征总是与目标分类一致（即它是一个强大的标识指标），足以表征数据。另一方面，如果特征总是具有相同的值，那么它的预测能力就会很低。</p>
<p>特征选择的一般机制是找到某种评估函数，当给定一个潜在特征时，它返回特征有用程度的评分，然后保留评估最高 的特征。这些方法可能具有不检测特征之间的相关性的缺点。其它方法可能会更加暴力：尝试原始特征列表的所有可能的子集，在每个组合上训练算法，保留结果最好的组合。</p>
<p>我们可以统计对两个随机变量（给定特征和目标值）的无关性来对评估函数进行评估；也就是说，它们之间是没有相关性的。</p>
<p>Sklearn在feature_selection模块中提供了多个方法。我们将使用SelectPercentile方法，它根据用户指定的百分比选择得分最高的特征。最流行的统计校验是卡方统计。我们用Titanic为例；选择最重要的20% 特征。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">feature_selection</span>
<span class="n">fs</span> <span class="o">=</span> <span class="n">feature_selection</span><span class="o">.</span><span class="n">SelectPercentile</span><span class="p">(</span><span class="n">feature_selection</span><span class="o">.</span><span class="n">chi2</span><span class="p">,</span><span class="n">percentile</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_train_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1"># 现在这个变量保存了统计意义上的最重要的特征。我们以它来训练决策树</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_fs</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">X_test_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_fs</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_fs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"精度：{0:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_fs</span><span class="p">)))</span>
</pre></div>
<div class="highlight"><pre><span></span>精度：0.848
</pre></div>
<p>精度有所提升。</p>
<p>是否能找到最理想的数量的特征呢？理想的情况下我们希望在训练集上得到最佳的性能，这也是可能通过暴力算法尝试所有不同数量的特征然后在训练集上使用交叉验证进行评估。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>

<span class="n">percentiles</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">fs</span> <span class="o">=</span> <span class="n">feature_selection</span><span class="o">.</span><span class="n">SelectPercentile</span><span class="p">(</span>
        <span class="n">feature_selection</span><span class="o">.</span><span class="n">chi2</span><span class="p">,</span><span class="n">percentile</span><span class="o">=</span><span class="n">i</span>
    <span class="p">)</span>
    <span class="n">X_train_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span><span class="n">X_train_fs</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results</span><span class="p">,</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1">#print results    </span>
<span class="n">optimal_percentil</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">results</span> <span class="o">==</span> <span class="n">results</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span> <span class="n">optimal_percentil</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"理想的特征数量："</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">percentiles</span><span class="p">[</span><span class="n">optimal_percentil</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>

<span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">pl</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Number of feature selected"</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Cross-validation accuracy"</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">percentiles</span><span class="p">,</span><span class="n">results</span><span class="p">)</span>
<span class="n">pl</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="k">[13]</span>
<span class="na">理想的特征数量：</span>
<span class="na">66</span>
</pre></div>
<p><img alt="png" src="./learning_sklearn/chap4_files/chap4_17_1.png"/></p>
<p>可以看到在开始阶段添加特征时精度快速提升，到超过6之后出现波动。我们可以观察它是否在测试集上能提升性能。</p>
<div class="highlight"><pre><span></span><span class="n">fs</span> <span class="o">=</span> <span class="n">feature_selection</span><span class="o">.</span><span class="n">SelectPercentile</span><span class="p">(</span>
    <span class="n">feature_selection</span><span class="o">.</span><span class="n">chi2</span><span class="p">,</span>
    <span class="n">percentile</span> <span class="o">=</span> <span class="n">percentiles</span><span class="p">[</span><span class="n">optimal_percentil</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">X_train_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_fs</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">X_test_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_fs</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_fs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"精度：{0:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_fs</span><span class="p">)))</span>
</pre></div>
<div class="highlight"><pre><span></span>精度：0.845
</pre></div>
<p>性能提升明显。</p>
<p>前面在构建分类器时，使用了默认参数，除了criterion(分裂标准) 参数使用的是entropy。我们可以使用不同的参数来进行提升？这项任务被称为模型选择。这里我们先使用gini来代替entropy。</p>
<div class="highlight"><pre><span></span><span class="n">dt</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">'entropy'</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span><span class="n">X_train_fs</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Entropy分裂：{0:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="n">dt</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">'gini'</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span><span class="n">X_train_fs</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Gini分裂：{0:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_fs</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">X_test_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_fs</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_fs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"精度{0:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_fs</span><span class="p">)))</span>
</pre></div>
<div class="highlight"><pre><span></span>Entropy分裂：0.874
Gini分裂：0.879
精度0.845
</pre></div>
<p>修改模型可以在测试集上得到更好的性能，但我们不能在测试集上评估，因为测试集数据都不是模型&ldquo;未见过&rdquo;的数据。</p>
<h2>模型选择</h2>
<p>前一节我们在数据预处理环节，选择最重要的特征。选择特征是决定性的环节。现在我们聚焦于另一个重要的步骤：算法参数，即超参数（区别于机器学习算法内部的参数）。许多机器学习算法都有超参数，它用于指引下层的方法并对结果产生重大的影响。本节我们将回顾一些方法用于获取最佳的参数配置，这个过程被称为模型选择。</p>
<p>在第二章的文本分类问题中。我们介绍使用TF-IDF向量和多项式贝叶斯算法对新闻进行分类。MultinomialNB算法有一个重要的参数，alpha参数。最初我们使用了它的默认值（alpha=1.0）得到的精度是0.89。当将它设置为0.01时得到的精度提升至了0.92。配置这个参数对这个算未予的性能影响非常大。我们怎样才能知道0.01这个参数值更好呢？我们可以尝试其它可能值，获得到更好的效果。</p>
<p>这里我们使用3000个样本。</p>
<div class="highlight"><pre><span></span><span class="c1">#%pylab inline</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">news</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
</pre></div>
<p>然后导入停用词并创建组合TF-IDF向量和算法和管道。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_stop_words</span><span class="p">():</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'stopwords_en.txt'</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="n">result</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">stop_words</span> <span class="o">=</span> <span class="n">get_stop_words</span><span class="p">()</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">'vect'</span><span class="p">,</span><span class="n">TfidfVectorizer</span><span class="p">(</span>
    <span class="n">stop_words</span> <span class="o">=</span> <span class="n">stop_words</span><span class="p">,</span>
    <span class="n">token_pattern</span> <span class="o">=</span> <span class="sa">ur</span><span class="s2">"\b[a-z0-9_\-\.]+[a-z0-9_\-\.]+\b"</span><span class="p">)),</span>
               <span class="p">(</span><span class="s1">'nb'</span><span class="p">,</span><span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">))])</span>
</pre></div>
<p>使用三折交叉验证法：</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span><span class="n">KFold</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">sem</span>

<span class="k">def</span> <span class="nf">evaluate_cross_validation</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
    <span class="c1"># 创建k折交叉验证</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="n">K</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># 默认的score返回评估器的score（精度）方法返回的值</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"均值：{0:.3f}(+/-{1:.3f})"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span><span class="n">sem</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>

<span class="n">evaluate_cross_validation</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="k">[ 0.812  0.806  0.831]</span>
<span class="na">均值：0.816(+/-0.008)</span>
</pre></div>
<p>看起来我们应该使用一些不同的参数来训练算法，并保留获取最佳结果的参数。下面的函数将使用一组值来训练算法，每次使用k折交叉验证法计算精度。之后它绘制出训练集和测试集上的得分。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_params</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">param_values</span><span class="p">,</span><span class="n">param_name</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
    <span class="c1"># 将训练和测试分值初始化为0</span>
    <span class="n">train_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param_values</span><span class="p">))</span>
    <span class="n">test_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param_values</span><span class="p">))</span>

    <span class="c1"># 遍历所有参数值</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_values</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'{} = {} '</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span><span class="n">param_value</span><span class="p">))</span>
        <span class="c1"># 设置分类器参数</span>
        <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">param_name</span><span class="p">:</span><span class="n">param_value</span><span class="p">})</span>
        <span class="c1"># 初始化每折验证的K值</span>
        <span class="n">k_train_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="n">k_test_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="c1"># 创建K折交叉验证</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 遍历所有K 折</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="p">):</span>
            <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">train</span><span class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
            <span class="n">k_train_scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">train</span><span class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
            <span class="n">k_test_scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">test</span><span class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>

        <span class="n">train_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">k_train_scores</span><span class="p">)</span>
        <span class="n">test_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">k_test_scores</span><span class="p">)</span>

    <span class="c1"># 绘制训练集测试集分值</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">param_values</span><span class="p">,</span><span class="n">train_scores</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">param_values</span><span class="p">,</span><span class="n">test_scores</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'g'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Alpha values"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Mean Cross-validation accuracy"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_scores</span><span class="p">,</span><span class="n">test_scores</span>
</pre></div>
<p>这个函数接收6个参数：特征数组，目标数组，分类器对象，参数列表，参数名称，交叉验证次数k的值。</p>
<p>接下来调用函数，我们使用numpy的logspace函数在对数空间上生成alpha值列表。</p>
<div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>[  1.00000000e-07   1.00000000e-06   1.00000000e-05   1.00000000e-04
   1.00000000e-03   1.00000000e-02   1.00000000e-01   1.00000000e+00]
</pre></div>
<p>我们将这些值作为pipeline中朴素贝叶斯分类器的alpha参数，它的参数名称为nb__alpha。我们使用3折交叉验证。</p>
<div class="highlight"><pre><span></span><span class="n">train_scores</span><span class="p">,</span><span class="n">test_scores</span> <span class="o">=</span> <span class="n">calc_params</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">alphas</span><span class="p">,</span><span class="s1">'nb__alpha'</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>nb__alpha = 1e-07 
nb__alpha = 1e-06 
nb__alpha = 1e-05 
nb__alpha = 0.0001 
nb__alpha = 0.001 
nb__alpha = 0.01 
nb__alpha = 0.1 
nb__alpha = 1.0
</pre></div>
<p><img alt="png" src="./learning_sklearn/chap4_files/chap4_33_1.png"/></p>
<p>可以看到与我们预计的情况一样，训练集上的精度总是大于测试集上的精度。我们可以看到上图中最高的测试集精度的alpha值在 <span class="math">\(10^{-2}\)</span> 和 <span class="math">\(10^{-1}\)</span> 之间。在这个区间分类器显示出过拟合（训练集精度高但测试集精度低）。在这个区间之上，分类器则欠拟合（训练集精度过低）。</p>
<p>值得注意的是可以在 <span class="math">\(10^(-2)\)</span> 和 <span class="math">\(10^(-1)之间找到最佳的alpha值\)</span>。</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">"train scores：{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_scores</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"test scores: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_scores</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span>train scores：[ 1.          1.          1.          1.          1.          1.
  0.99366667  0.93533333]
test scores: [ 0.77433333  0.78333333  0.79066667  0.802       0.812       0.81633333
  0.783       0.623     ]
</pre></div>
<p>最佳的alpha值是0.1（得到的精度是0.812）。</p>
<p>上面我们创建了一个非常有用的函数绘制和评估分类吕的最佳参数值。接下来让我们调整另一个分类器，使用SVM代替NB。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">'vect'</span><span class="p">,</span><span class="n">TfidfVectorizer</span><span class="p">(</span>
        <span class="n">stop_words</span> <span class="o">=</span> <span class="n">stop_words</span><span class="p">,</span>
        <span class="n">token_pattern</span> <span class="o">=</span> <span class="sa">ur</span><span class="s2">"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b"</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'svc'</span><span class="p">,</span><span class="n">SVC</span><span class="p">())</span>
<span class="p">])</span>
<span class="err">`</span>
<span class="c1"># 在使用SVC 分类器前我们将使用 calc_params 函数调整它的 gamma 参数</span>

<span class="n">gammas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">train_scores</span><span class="p">,</span><span class="n">test_scores</span> <span class="o">=</span> <span class="n">calc_params</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">gammas</span><span class="p">,</span><span class="s1">'svc__gamma'</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>svc__gamma = 0.01 
svc__gamma = 0.1 
svc__gamma = 1.0 
svc__gamma = 10.0
</pre></div>
<p><img alt="png" src="./learning_sklearn/chap4_files/chap4_37_1.png"/></p>
<p>最佳的gamma值是1，这个参数得到的训练集精度是0.999测试集上的精度是0.760。</p>
<p>如果仔细看SVC类的构建器参数，我们可以找到其它参数，除了gamma外，它们也可以影响分类效果。如果我们只调整gamma值，我们隐式的得到最优的C参数值为1.0（默认值我们没有显式的指定）。也许我们使用另一对C和gamma的组合能得到更好的结果。这增加了新的复杂度，我们应该尝试所有可能的组合并保留更好的值。</p>
<h2>网格搜索(Grid Search)</h2>
<p>为处理这个问题，<code>sklarn.grid_search</code> 模块提供了GridSearchCV类。前面我们的 <code>calc_params</code> 函数也是一个一维的网格搜索。使用GridSearchCV，我们可以指定一个包含任意数量的参数和参数值的网格并进行遍历。它将使用所有可能的参数组合训练分类器，并使用交叉难器计算每种组合。</p>
<p>我们使用C和gamma参数进行调优。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'svc__gamma'</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span>
             <span class="s1">'svc__C'</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)}</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">'vect'</span><span class="p">,</span><span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span> <span class="o">=</span> <span class="n">stop_words</span><span class="p">,</span>
                            <span class="n">token_pattern</span> <span class="o">=</span> <span class="sa">ur</span><span class="s2">"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b"</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'svc'</span><span class="p">,</span><span class="n">SVC</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">parameters</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">refit</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="o">%</span><span class="n">time</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span><span class="n">gs</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
<div class="highlight"><pre><span></span>Fitting 3 folds for each of 12 candidates, totalling 36 fits
[CV] svc__gamma=0.01, svc__C=0.1 .....................................
[CV] ............................ svc__gamma=0.01, svc__C=0.1 -  11.2s
[CV] svc__gamma=0.01, svc__C=0.1 .....................................


[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.2s remaining:    0.0s


[CV] ............................ svc__gamma=0.01, svc__C=0.1 -  10.8s
[CV] svc__gamma=0.01, svc__C=0.1 .....................................
[CV] ............................ svc__gamma=0.01, svc__C=0.1 -  10.9s
[CV] svc__gamma=0.1, svc__C=0.1 ......................................
[CV] ............................. svc__gamma=0.1, svc__C=0.1 -  11.0s
[CV] svc__gamma=0.1, svc__C=0.1 ......................................
[CV] ............................. svc__gamma=0.1, svc__C=0.1 -  11.6s
[CV] svc__gamma=0.1, svc__C=0.1 ......................................
[CV] ............................. svc__gamma=0.1, svc__C=0.1 -  10.9s
[CV] svc__gamma=1.0, svc__C=0.1 ......................................
[CV] ............................. svc__gamma=1.0, svc__C=0.1 -  11.4s
[CV] svc__gamma=1.0, svc__C=0.1 ......................................
[CV] ............................. svc__gamma=1.0, svc__C=0.1 -  11.5s
[CV] svc__gamma=1.0, svc__C=0.1 ......................................
[CV] ............................. svc__gamma=1.0, svc__C=0.1 -  11.1s
[CV] svc__gamma=10.0, svc__C=0.1 .....................................
[CV] ............................ svc__gamma=10.0, svc__C=0.1 -  11.1s
[CV] svc__gamma=10.0, svc__C=0.1 .....................................
[CV] ............................ svc__gamma=10.0, svc__C=0.1 -  11.1s
[CV] svc__gamma=10.0, svc__C=0.1 .....................................
[CV] ............................ svc__gamma=10.0, svc__C=0.1 -  11.0s
[CV] svc__gamma=0.01, svc__C=1.0 .....................................
[CV] ............................ svc__gamma=0.01, svc__C=1.0 -  11.0s
[CV] svc__gamma=0.01, svc__C=1.0 .....................................
[CV] ............................ svc__gamma=0.01, svc__C=1.0 -  10.9s
[CV] svc__gamma=0.01, svc__C=1.0 .....................................
[CV] ............................ svc__gamma=0.01, svc__C=1.0 -  11.0s
[CV] svc__gamma=0.1, svc__C=1.0 ......................................
[CV] ............................. svc__gamma=0.1, svc__C=1.0 -  11.1s
[CV] svc__gamma=0.1, svc__C=1.0 ......................................
[CV] ............................. svc__gamma=0.1, svc__C=1.0 -  10.9s
[CV] svc__gamma=0.1, svc__C=1.0 ......................................
[CV] ............................. svc__gamma=0.1, svc__C=1.0 -  11.0s
[CV] svc__gamma=1.0, svc__C=1.0 ......................................
[CV] ............................. svc__gamma=1.0, svc__C=1.0 -  11.1s
[CV] svc__gamma=1.0, svc__C=1.0 ......................................
[CV] ............................. svc__gamma=1.0, svc__C=1.0 -  11.0s
[CV] svc__gamma=1.0, svc__C=1.0 ......................................
[CV] ............................. svc__gamma=1.0, svc__C=1.0 -  11.1s
[CV] svc__gamma=10.0, svc__C=1.0 .....................................
[CV] ............................ svc__gamma=10.0, svc__C=1.0 -  11.2s
[CV] svc__gamma=10.0, svc__C=1.0 .....................................
[CV] ............................ svc__gamma=10.0, svc__C=1.0 -  11.3s
[CV] svc__gamma=10.0, svc__C=1.0 .....................................
[CV] ............................ svc__gamma=10.0, svc__C=1.0 -  11.1s
[CV] svc__gamma=0.01, svc__C=10.0 ....................................
[CV] ........................... svc__gamma=0.01, svc__C=10.0 -  10.7s
[CV] svc__gamma=0.01, svc__C=10.0 ....................................
[CV] ........................... svc__gamma=0.01, svc__C=10.0 -  10.9s
[CV] svc__gamma=0.01, svc__C=10.0 ....................................
[CV] ........................... svc__gamma=0.01, svc__C=10.0 -  10.8s
[CV] svc__gamma=0.1, svc__C=10.0 .....................................
[CV] ............................ svc__gamma=0.1, svc__C=10.0 -  11.1s
[CV] svc__gamma=0.1, svc__C=10.0 .....................................
[CV] ............................ svc__gamma=0.1, svc__C=10.0 -  11.0s
[CV] svc__gamma=0.1, svc__C=10.0 .....................................
[CV] ............................ svc__gamma=0.1, svc__C=10.0 -  11.0s
[CV] svc__gamma=1.0, svc__C=10.0 .....................................
[CV] ............................ svc__gamma=1.0, svc__C=10.0 -  11.2s
[CV] svc__gamma=1.0, svc__C=10.0 .....................................
[CV] ............................ svc__gamma=1.0, svc__C=10.0 -  11.2s
[CV] svc__gamma=1.0, svc__C=10.0 .....................................
[CV] ............................ svc__gamma=1.0, svc__C=10.0 -  11.5s
[CV] svc__gamma=10.0, svc__C=10.0 ....................................
[CV] ........................... svc__gamma=10.0, svc__C=10.0 -  11.6s
[CV] svc__gamma=10.0, svc__C=10.0 ....................................
[CV] ........................... svc__gamma=10.0, svc__C=10.0 -  11.1s
[CV] svc__gamma=10.0, svc__C=10.0 ....................................
[CV] ........................... svc__gamma=10.0, svc__C=10.0 -  11.1s
CPU times: user 6min 26s, sys: 5.85 s, total: 6min 32s
Wall time: 6min 39s


[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  6.7min finished





({'svc__C': 10.0, 'svc__gamma': 0.10000000000000001}, 0.819)
</pre></div>
<p>使用网格搜索，我们找到了最佳的C和gamma参数组合，10.0和0.10，其3折交叉难的精度为0.811，比之前只优化gamma参数时C默认取1.0时得到的更好一些（当时的精度为0.76）。</p>
<p>现在，我们可以继续尝试调整其它参数，不仅限于SVC的参数，也可以调整TfidfVectorizer的参数，因为它也是评估器的一部分。这又将增加更多的复杂性。前面的网络搜索已经需要花超过5分钟的时间来运行了。这种方法是需要消耗大量的资源和时间的；这也是我们只使用整个训练集的一个子集的原因。</p>
<h2>并行网格搜索</h2>
<p>网格搜索的计算量是按要调优的参数的数量呈几何级数增长的。我们可以通过让每种参数组合以并行的方式运行就可以提高响应的速度。前面的例子中，我们有4个gamma值和3个C值共12种参数组合。另外，我们还需要训练每种组合三次（3折交叉验证）。我们可以将这36个任务并行，因为每个任务都是独立的。</p>
<p>现代的计算机都有多个CPU核心可以并行运行任务。在IPython中也有一个非常有用的工具IPython parallel，它允许我们以并行的方式运行多个任务，每个任务使用机器的一个CPU核心。</p>
<p>首先，我们需要定义一下新的函数将K折交叉验证的数据放在不同的文件中。这些文件将被运行对应折次的进程所加载。我们使用joblib来实现。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.externals</span> <span class="kn">import</span> <span class="n">joblib</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">persist_cv_splits</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">'data'</span><span class="p">,</span>
                      <span class="n">suffix</span><span class="o">=</span><span class="s1">'_cv_</span><span class="si">%03d</span><span class="s1">.pkl'</span><span class="p">):</span>
    <span class="sd">"""Dump K folds to filesystem."""</span>

    <span class="n">cv_split_filenames</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># 创建K折交叉验证</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># 遍历 K 折</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,(</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="p">):</span>
        <span class="n">cv_fold</span> <span class="o">=</span> <span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">train</span><span class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span>
                   <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">test</span><span class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
        <span class="n">cv_split_filename</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="n">suffix</span> <span class="o">%</span> <span class="n">i</span>
        <span class="n">cv_split_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">cv_split_filename</span><span class="p">)</span>
        <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">cv_fold</span><span class="p">,</span><span class="n">cv_split_filename</span><span class="p">)</span>
        <span class="n">cv_split_filenames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_split_filename</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cv_split_filenames</span>

<span class="n">cv_filenames</span> <span class="o">=</span> <span class="n">persist_cv_splits</span><span class="p">(</span><span class="n">news</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">news</span><span class="o">.</span><span class="n">target</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">'news'</span><span class="p">)</span>
</pre></div>
<p>下面的函数加载某个指定的fold并将它送给设置了特定参数的分类器，返回测试结果。这个函数将被每个并行任务所调用。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_evaluation</span><span class="p">(</span><span class="n">cv_split_filename</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">params</span><span class="p">):</span>
    <span class="c1"># 应该在工作进程的空间导入模块</span>
    <span class="kn">from</span> <span class="nn">sklearn.externals</span> <span class="kn">import</span> <span class="n">joblib</span>

    <span class="c1"># 从文件系统加载 fold 的训练和测试分块</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
        <span class="n">cv_split_filename</span><span class="p">,</span><span class="n">mmap_mode</span><span class="o">=</span><span class="s1">'c'</span><span class="p">)</span>

    <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">test_score</span>
</pre></div>
<p>最终由下面的函数在并行任务中执行网格搜索。对于每个参数组合（由IterGrid返回），它遍历K folds并创建任务进行计算。它返回参数组合和任务列表。</p>
<div class="highlight"><pre><span></span><span class="c1"># from sklearn.grid_search import IterGrid</span>
<span class="c1"># http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.ParameterGrid.html#sklearn.grid_search.ParameterGrid</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ParameterGrid</span>

<span class="k">def</span> <span class="nf">parallel_grid_search</span><span class="p">(</span><span class="n">lb_view</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">cv_split_filenames</span><span class="p">,</span><span class="n">param_grid</span><span class="p">):</span>
    <span class="n">all_tasks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_parameters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ParameterGrid</span><span class="p">(</span><span class="n">param_grid</span><span class="p">))</span>

    <span class="c1"># 遍历所有参数组合</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_parameters</span><span class="p">):</span>
        <span class="n">task_for_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">cv_split_filename</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv_split_filenames</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">lb_view</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
                <span class="n">compute_evaluation</span><span class="p">,</span><span class="n">cv_split_filename</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span>
                <span class="n">params</span><span class="p">)</span>
            <span class="n">task_for_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="n">all_tasks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">task_for_params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_parameters</span><span class="p">,</span><span class="n">all_tasks</span>    
</pre></div>
<p>现在我们使用IPython Parallel获取客户端和负责均衡view(lb_view)。我们必须先使用IPython Notebook的Cluster 页签，创建一个本部N引擎的集群（每个对应于机器上的一个CPU核）。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="c1">#from IPython.parallel import Client</span>
<span class="c1"># 上面的功能已经变化</span>
<span class="c1"># 参见https://github.com/ipython/ipyparallel</span>
<span class="c1"># 需要安装ipyparallel并运行</span>
<span class="c1"># ipcluster nbextension enable 启动ipython notebook的cluster 页签</span>
<span class="c1"># ipcluster start 启动并行支持</span>
<span class="kn">from</span> <span class="nn">ipyparallel</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
<span class="n">lb_view</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">load_balanced_view</span><span class="p">()</span>

<span class="n">all_parameters</span><span class="p">,</span><span class="n">all_tasks</span> <span class="o">=</span> <span class="n">parallel_grid_search</span><span class="p">(</span>
    <span class="n">lb_view</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">cv_filenames</span><span class="p">,</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
<p>IPython parallel将以并行方式运行任务。我们可以它监控整个任务组的进行进度。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_progress</span><span class="p">(</span><span class="n">tasks</span><span class="p">):</span>
    <span class="n">progress</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">task</span><span class="o">.</span><span class="n">ready</span><span class="p">()</span> <span class="k">for</span> <span class="n">task_group</span> <span class="ow">in</span> <span class="n">tasks</span> <span class="k">for</span> <span class="n">task</span> <span class="ow">in</span> <span class="n">task_group</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"任务完成情况:{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">progress</span><span class="p">))</span>

<span class="n">print_progress</span><span class="p">(</span><span class="n">all_tasks</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="err">任务完成情况</span><span class="o">:</span><span class="mf">100.0</span>
</pre></div>
<p>我们可以定义函数计算已完成任务的平均分值。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_bests</span><span class="p">(</span><span class="n">all_parameters</span><span class="p">,</span><span class="n">all_tasks</span><span class="p">,</span><span class="n">n_top</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="sd">"""计算已完成任务的平均分值"""</span>

    <span class="n">mean_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">param</span><span class="p">,</span><span class="n">task_group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_parameters</span><span class="p">,</span><span class="n">all_tasks</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">task_group</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">ready</span><span class="p">()]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">mean_scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span><span class="n">param</span><span class="p">))</span>

    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">mean_scores</span><span class="p">,</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">n_top</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">find_bests</span><span class="p">(</span><span class="n">all_parameters</span><span class="p">,</span><span class="n">all_tasks</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span>[(0.053666666666666668, {'svc__gamma': 0.01, 'svc__C': 0.10000000000000001})]
</pre></div>
<h2>小结</h2>
<p>本章回顾了机器学习中两个重要的提高效果的方法：特征选择和模型选择。首先，我们使用不同的技术进行数据预处理，特征抽取，并选择最有价值的特征。然后我们使用技术自动计算最有价值的超参数，并使用并行计算提升性能。</p>
<p>读者需要了解本书只包含部分机器学习方法。还有大量监督和非监督学习方法。比如：</p>
<ul>
<li>
<p>半监督学习算法，介于监督学习和非监督学习之间。它将少量标记过的数据与未标记的数据进混合在一起。通常未标记的数据可以由下层标记过的数据揭示出数据的分布。</p>
</li>
<li>
<p>Active learning是半监督学习的一个特殊类型。它在难以提供标记数据时非常有效。在active learning中算法积极的请求人类专家回答某些未分类样本的标签，然后学习这些标签样本的特征。</p>
</li>
<li>
<p>强化学习通过反馈信息提高学习效果，它的代理从反馈中进行学习。代理尝试最大化累积奖励来执行任务。这些方法在机器人和视频游戏方面非常成功。</p>
</li>
<li>
<p>顺序分类（常用于自然语言处理（NLP））将一系列标签分配给一系列项目; 例如，一个句子中词语的词性。</p>
</li>
</ul>
<p>除了这些，还有很多监督学习方法与我们提出的方法截然不同。 例如神经网络，最大熵模型，基于记忆的模型和基于规则的模型。 机器学习是一个非常活跃的研究领域，不断增长的文献， 有很多书籍和课程可供读者深入理论和细节。</p>
<p>Sklearn 已经实现这些算法中的许多种，缺少部分算法，但期望它的积极和热心的贡献者能建立它们。 我们鼓励读者成为社区的一员！</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

      </div><!-- /.entry-content -->
      <div class="comments">
        <h2>Comments</h2>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          var disqus_identifier = "learning-sklearnbi-ji-si.html";
          (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//jamsa-github-io.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
      </div>

    </article>
  </section>
</div>
                </div>
            </div>
        </div>

        <footer id="site-footer">
            <div class="row-fluid">
                <div class="span10 offset1">
                    <address>
                        <p>
                        This blog is proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                        </p>
                        <p>
                            <a href="http://github.com/jsliang/pelican-fresh/">Fresh</a> is a responsive theme designed by <a href="http://jsliang.com/">jsliang</a> and <a href="https://github.com/jsliang/pelican-fresh/graphs/contributors">contributors</a>.
                        Special thanks to <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a> and <a href="http://getbootstrap.com/">Twitter Bootstrap</a>.
                        </p>
                    </address>
                </div>
            </div>
        </footer>

<script type="text/javascript">
    var disqus_shortname = 'jamsa-github-io';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

        <link href="http://apps.bdimg.com/libs/highlight.js/9.1.0/styles/default.min.css" rel="stylesheet">
        <script src="http://apps.bdimg.com/libs/highlight.js/9.1.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
</body>
</html>