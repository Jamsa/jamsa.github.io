<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Jamsa" />
        <meta name="robots" content="index, follow"/>

        <meta property="og:title" content="梯度的理解"/>
        <meta property="og:url" content="./ti-du-de-li-jie.html"/>
        <meta property="og:site_name" content="Jamsa的笔记"/>
        <meta property="og:type" content="article"/>

        <link rel="canonical" href="./ti-du-de-li-jie.html" />

        <title>梯度的理解 | Jamsa的笔记</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" />
        <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" />
        

        <link rel="stylesheet" type="text/css" href="./theme/css/main.css" />

        <script type="text/javascript">var switchTo5x=true;</script>
        <script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
        <script type="text/javascript">
         stLight.options({
             publisher: "",
             doNotHash: false,
             doNotCopy: false,
             hashAddressBar: false
         });
        </script>
    </head>

    <body id="index">
        <div class="row-fluid">
            <div class="span10 offset1">
                <header id="banner" >
                    <h1>
                        <a href="./">Jamsa的笔记 </a>
                    </h1>
                    <nav class="navbar">
                        <div class="navbar-inner">
                            <ul class="nav">
                                <li ><a href="./category/da-shu-ju.html">大数据</a></li>
                                <li ><a href="./category/fang-fa.html">方法</a></li>
                                <li class="active"><a href="./category/ji-qi-xue-xi.html">机器学习</a></li>
                                <li ><a href="./category/kai-fa.html">开发</a></li>
                                <li ><a href="./category/qian-duan.html">前端</a></li>
                                <li ><a href="./category/xiao-lu.html">效率</a></li>
                                <li ><a href="./category/yi-dong.html">移动</a></li>
                            </ul>

                        </div>
                    </nav>
                </header><!-- /#banner -->
            </div>
        </div>

        <div class="row-fluid">
            <div class="span10 offset1">
                <div class="row-fluid">
<div class="span10 offset1">
  <section>
    <article>
      <header>
        <h1 class="entry-title">
          <a href="./ti-du-de-li-jie.html" rel="bookmark"
             title="Permalink to 梯度的理解">梯度的理解</a></h1>
      </header>
      <div class="entry-content">
<footer class="post-info">
    <address class="vcard author">
        by <a class="url fn" href="./author/jamsa.html">Jamsa</a>
    </address>

    in <a href="./category/ji-qi-xue-xi.html">机器学习</a>

    on 2018-05-12

        |
        tags:         <a href="./tag/python.html">python</a>
        <a href="./tag/machine-learn.html">machine learn</a>


        |
        <a href="./ti-du-de-li-jie.html#disqus_thread">comments</a>

    
</footer><!-- /.post-info -->

        <h1>梯度的理解</h1>
<p>参考以下两篇文章:</p>
<p><a href="https://www.zhihu.com/question/36301367">如何直观形象的理解方向导数与梯度以及它们之间的关系？</a>对导数和梯度的解释最为简明。</p>
<p><a href="https://blog.csdn.net/walilk/article/details/50978864">WangBo的机器学习乐园的博文</a></p>
<p>总结以下内容：</p>
<ul>
<li>导数：</li>
</ul>
<p>导数指的是一元函数在某点经轴正方向的变化率。</p>
<div class="math">$$f'(x) = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}$$</div>
<ul>
<li>偏导数：</li>
</ul>
<p>偏导数是多元函数在某点沿某个轴正方向的变化率。</p>
<div class="math">$$\frac{\partial f(x_0,x_1, \ldots, x_n) }{\partial x_j} = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0} \frac{f(x_0, \ldots, x_j + \Delta x, \ldots, x_n) - f(x_0, \ldots, x_j, \ldots, x_n)}{\Delta x}$$</div>
<ul>
<li>方向导数：</li>
</ul>
<p>导数和偏导数都是沿某轴的正方向变化。任意方向变化率就是方向导数。即：某一点在某一趋近方向上的导数值。</p>
<div class="math">$$\frac{\partial f(x_0,x_1, \ldots, x_n) }{\partial l } = \lim_{\rho x \to 0} \frac{\Delta y}{\Delta x} = \lim_{\rho x \to 0} \frac{f(x_0, \ldots, x_j + \Delta x, \ldots, x_n) - f(x_0, \ldots, x_j, \ldots, x_n)}{\rho}$$</div>
<ul>
<li>梯度</li>
</ul>
<p>函数在某点的梯度是一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。</p>
<div class="math">$$gradf(x_0, x_1, \ldots, x_n) = (\frac{\partial f}{\partial x_0}, \ldots, \frac{\partial f}{\partial x_j}, \ldots, \frac{\partial f}{\partial x_n})$$</div>
<p>梯度是偏导的集合。</p>
<ul>
<li>梯度下降</li>
</ul>
<p>在每个变量轴减小对应的变量值（学习率*轴的偏导值），可描述为：</p>
<div class="math">\begin{equation} 
x_0 = x_0 - \alpha \frac{\partial f}{\partial x_0} \\\\
\ldots \ldots \ldots \\\\
x_j = x_j - \alpha \frac{\partial f}{\partial x_j} \\\\
\ldots \ldots \ldots \\\\
x_n = x_n - \alpha \frac{\partial f}{\partial x_n} \\\\
\end{equation}</div>
<h2>一元情况下的示例</h2>
<p>以这段简单的<a href="https://github.com/hunkim/PyTorchZeroToAll/blob/master/02_manual_gradient.py">手动求导的pytorch代码</a>为例:</p>
<div class="highlight"><pre><span></span><span class="n">x_data</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]</span>

<span class="n">w</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># a random guess: random value</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>


<span class="c1"># Loss function</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>


<span class="c1"># compute gradient</span>
<span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1"># d_loss/d_w</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<p>代码中 <span class="math">\(x\)</span> 为输入 <span class="math">\(y\)</span> 为输出，权值为 <span class="math">\(w\)</span> 没有偏值项 <span class="math">\(b\)</span> 。网络的前向传播公式为 <span class="math">\(xw\)</span> 。损失函数<code>loss</code>为 <span class="math">\(f(x,y) = (xw -y)^2\)</span> ，即取误差平方。由于w是标量，梯度计算就变成对 <span class="math">\(w\)</span> 求偏导，即为一元函数的求导:</p>
<div class="math">$$f(w) = (xw)^2 + y^2 - 2(xw)y$$</div>
<div class="math">$${\partial f(w) \over \partial w} = 2xw - 2xy = 2x(xw-y)$$</div>
<p>即代码中的<code>gradient</code>函数。</p>
<h1>梯度下降算法</h1>
<h2>数学推导</h2>
<p><a href="https://blog.csdn.net/yhao2014/article/details/51554910">参考</a></p>
<p>设拟合函数或神经网络的前向传播函数为<span class="math">\(h(\theta)\)</span>:</p>
<div class="math">$$h(\theta) = \theta_0 + \theta_1 x_1 + \ldots + \theta_n x_n = \sum_{j=0}^n \theta_n x_n$$</div>
<p>其向量形式：</p>
<div class="math">$$h_\theta (x) = \theta^T X$$</div>
<p>损失函数为：</p>
<div class="math">$$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta (x^i) - y^i)^2$$</div>
<p>优化目标为最小化损失函数。</p>
<h3 id="pi-liang-ti-du-xia-jiang-bgdsuan-fa-tui-dao">批量梯度下降BGD算法推导</h3>
<p>对每个<span class="math">\(\theta_j\)</span>求偏导，得到每个<span class="math">\(\theta_j\)</span>的梯度：</p>
<div class="math">$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i = 1}^m (h_\theta (x^i) - y^i) x_j^i$$</div>
<p>优化参数的过程就是按每个参数的负梯度方向方向来更新每个<span class="math">\(\theta_j\)</span>，其中的<span class="math">\(\alpha\)</span>表示步长（学习率）：</p>
<div class="math">$$\theta_j = \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j} = \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta (x^i) - y^i) x_j^i$$</div>
<p>由于BGD算法需要使用所有训练集数据，如果样本数量很多（即m很大），这种计算会非常耗时。所以就引入了随机梯度下降SGD算法。</p>
<h3 id="sui-ji-ti-du-xia-jiang-sgd">随机梯度下降SGD</h3>
<p>先将损失函数进行改写：</p>
<div class="math">$$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta (x^i) - y^i)^2 = \frac{1}{m} \sum_{i=1}^m cost(\theta , (x^i,y^i))$$</div>
<p>其中的<span class="math">\(cost(\theta , (x^i, y^i)) = \frac{1}{2}(h_\theta (x^i) - y^i)^2\)</span>称为样本点<span class="math">\((x^i, y^i)\)</span>的损失函数。这样就将问题转化为了对单个样本点的优化问题。</p>
<p>对这个新的损失函数求偏导，得到每个<span class="math">\(\theta_j\)</span>的梯度</p>
<div class="math">$$\frac{\partial cost(\theta, (x^i, y^i))}{\partial \theta_j} = (h_\theta (x^i) - y^i)x^i$$</div>
<p>然后根据这个梯度的负方向来更新每个<span class="math">\(\theta_j\)</span></p>
<div class="math">$$\theta_j = \theta_j - \alpha \frac{\partial cost(\theta , (x^i, y^i))}{\partial \theta_j} = \theta_j - \alpha (h_\theta (x^i) - y^i) x^i$$</div>
<p>随机梯度下降每次迭代只计算一个柆，能大大减少计算量。缺点是SGD并不是每次迭代都会向着整体最优化方向，并且最终得到的解不一定是全局最优解，而只是局部最优解。最终结果往往是在全局最优解附近。</p>
<h1>求导与神经网络的反向传播</h1>
<p><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">参考</a></p>
<p>即采用链式求导法逐层求导。</p>
<h2>pytorch中的自动求导功能</h2>
<p><a href="https://blog.csdn.net/manong_wxd/article/details/78734358">参考</a></p>
<p>pytorch中的自动求导机制是因为它对历史信息保存了记录。每个变更都有一个<code>.creator</code>属性，它指向把它作为输出的函数。这是一个由<code>Function</code>对象作为节点组成的有向无环图（DAG）的入口点，它们之间的引用就是图的边。每次执行一个操作时，一个表示它的新<code>Function</code>对象就被实例化，它的<code>forward()</code>方法被调用，并且它输出的<code>Variable</code>的创建者被设置为这个函数。然后，通过跟踪从任何变量到叶节点的路径，可以重建创建数据的操作序列，并自动计算梯度。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

      </div><!-- /.entry-content -->
      <div class="comments">
        <h2>Comments</h2>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          var disqus_identifier = "ti-du-de-li-jie.html";
          (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//jamsa-github-io.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
      </div>

    </article>
  </section>
</div>
                </div>
            </div>
        </div>

        <footer id="site-footer">
            <div class="row-fluid">
                <div class="span10 offset1">
                    <address>
                        <p>
                        This blog is proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                        </p>
                        <p>
                            <a href="http://github.com/jsliang/pelican-fresh/">Fresh</a> is a responsive theme designed by <a href="http://jsliang.com/">jsliang</a> and <a href="https://github.com/jsliang/pelican-fresh/graphs/contributors">contributors</a>.
                        Special thanks to <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a> and <a href="http://getbootstrap.com/">Twitter Bootstrap</a>.
                        </p>
                    </address>
                </div>
            </div>
        </footer>

<script type="text/javascript">
    var disqus_shortname = 'jamsa-github-io';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>

        <link href="http://apps.bdimg.com/libs/highlight.js/9.1.0/styles/default.min.css" rel="stylesheet">
        <script src="http://apps.bdimg.com/libs/highlight.js/9.1.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
         var _hmt = _hmt || [];
         (function() {
           var hm = document.createElement("script");
           hm.src = "https://hm.baidu.com/hm.js?5e20e9cdd1185d24ece1dae11118a04f";
           var s = document.getElementsByTagName("script")[0]; 
           s.parentNode.insertBefore(hm, s);
         })();
        </script>
</body>
</html>